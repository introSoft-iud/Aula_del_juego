{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Presentaci\u00f3n del curso","text":""},{"location":"#diplomado-en-construccion-de-aplicaciones-asistidas-por-ia","title":"Diplomado en construcci\u00f3n de Aplicaciones Asistidas por IA","text":"<p>Bienvenidos al diploma en construcci\u00f3n de aplicaciones asistidas por modelos de lenguaje de la IUDigital de Antioquia</p> <p>Aunque la inteligencia artificial ha existido como un campo exitoso y prometedor entre los expertos durante varias d\u00e9cadas, la llegada de capacidades computacionales m\u00e1s avanzadas \u2014ofrecidas por las GPU modernas\u2014 y las habilidades demostradas con el lanzamiento de ChatGPT fueron una gran sorpresa para muchos.</p> <p>No est\u00e1 del todo claro c\u00f3mo este \"peque\u00f1o\" avance en la escala de los modelos pudo desencadenar la gran cantidad de aplicaciones asistidas por IA que estamos viendo explotar cada semana. Lo que s\u00ed es claro es que el campo de la ingenier\u00eda de software est\u00e1 siendo revolucionado, y que el nuevo paradigma de construcci\u00f3n de software ya no consiste en los tradicionales flujos de ejecuci\u00f3n, sino que la nueva ingenier\u00eda de sistemas debe integrar a los LLMs en dichos flujos.</p> <p>Hemos dise\u00f1ado este diplomado para introducirte a este nuevo paradigma. </p>"},{"location":"#resultados-de-aprendizage","title":"Resultados de Aprendizage","text":""},{"location":"#pregunta-orientadora","title":"Pregunta Orientadora","text":""},{"location":"#mapa-del-curso","title":"Mapa del curso","text":"<p>crea a aqu\u00ed el mapa del curso</p>"},{"location":"#cronograma-de-actividades","title":"Cronograma de actividades","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Caracter\u00edsticas principales del reino Fungi Actividad de conocimientos previos Semana 1 10 % Principios de la teor\u00eda del aprendizaje Mapa mental sobre teor\u00eda del aprendizaje Semana 2 20 % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Total 100 %"},{"location":"#actividad-de-refuerzo","title":"Actividad de refuerzo","text":"<p>Agrega el nombre de la actividad de refuerzo (cuando as\u00ed se requiera).</p>"},{"location":"#configuracion-del-sistema-antes-de-comenzar","title":"Configuraci\u00f3n del sistema antes de comenzar","text":"<p>Antes de empezar a trabajar con los m\u00f3dulos del curso, debes configurar tu sistema para poder ejecutar los ejemplos correctamente.</p> <p>La forma m\u00e1s sencilla de hacerlo es descargando el archivo de configuraci\u00f3n \ud83d\udcc4environment.yml , el cual crear\u00e1 autom\u00e1ticamente un entorno de Conda llamado <code>DiplomadoIA_env</code> con todas las dependencias necesarias para el curso.</p>"},{"location":"#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Tener Anaconda instalado en tu computador.</li> <li>Usar una terminal Bash (en Windows puedes usar Anaconda Prompt, git bash, WSL o similares).</li> </ul>"},{"location":"#instalacion","title":"Instalaci\u00f3n","text":"<p>Una vez descargado el archivo de configuraci\u00f3n, ejecuta el siguiente comando en tu terminal:</p> <pre><code>conda env create -f environment.yml\n</code></pre>"},{"location":"#activacion-del-entorno","title":"Activaci\u00f3n del entorno","text":"<p>Para activar el entorno en tu terminal, ejecuta:</p> <p><pre><code>conda activate diplomado_IA\n</code></pre> A partir de aqu\u00ed, cualquier comando que ejecutes usar\u00e1 las dependencias definidas para el curso.</p> <p>Uso del entorno en Visual Studio Code</p> <p>Para tener en cuenta</p> <p>Para ejecutar notebooks <code>.ipynb</code> en Visual Studio Code usando este entorno:</p> <ol> <li>Abre VS Code.</li> <li>Abre la carpeta del proyecto o el notebook deseado.</li> <li>En la parte superior derecha del notebook, haz clic en la selecci\u00f3n de kernel.</li> <li>Elige el kernel correspondiente al entorno <code>diplomado_IA</code>. Si no aparece, reinicia VS Code o aseg\u00farate de haber activado el entorno desde la terminal integrada.</li> <li>Comienza a ejecutar celdas normalmente.</li> </ol> <p>Tip</p> <p>Puedes asegurarte de que el entorno se registre correctamente como kernel ejecutando en la terminal: <pre><code>python -m ipykernel install --user --name diplomado_IA --display-name \"Python (diplomado_IA)\"\n</code></pre></p>"},{"location":"Unidad%201/","title":"Unidad 1","text":""},{"location":"Unidad%201/#unidad-1-introduccion-a-la-construccion-de-aplicaciones-con-llms","title":"Unidad 1. Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLMs","text":""},{"location":"Unidad%201/#introduccion-a-la-unidad","title":"Introducci\u00f3n a la unidad","text":"<p>Bienvenidos a la primera unidad. En esta unidad, aprender\u00e1s de manera general c\u00f3mo funciona un modelo de lenguaje. Comenzaremos utilizando la API de OpenAI y exploraremos c\u00f3mo conectar sus modelos en aplicaciones. Luego, aprender\u00e1s a utilizar esta misma API a trav\u00e9s del framework LangChain. Introduciremos los aspectos fundamentales de la interacci\u00f3n con los LLMs usando LangChain: prompts, templates y output parsers.</p> <p>Como actividad pr\u00e1ctica, elaborar\u00e1s un sistema asistido por IA para extraer datos de comentarios de usuarios en un e-commerce.</p> <p>\u00a1Comencemos!</p>"},{"location":"Unidad%201/#resultados-de-aprendizaje","title":"Resultados de aprendizaje","text":"<p>Al finalizar esta unidad, estar\u00e1s en capacidad de realizar llamadas a los modelos de lenguaje de OpenAI a trav\u00e9s de la API para crear c\u00f3digo Python cuya ejecuci\u00f3n es asistida por LLMs.</p> <p>Aprender\u00e1s a configurar cadenas de ejecuci\u00f3n simples en LangChain usando LCEL, junto con prompt templates y output parsers, para convertir las salidas de los LLMs en objetos nativos de Python.</p>"},{"location":"Unidad%201/#cronograma-de-actividades-unidad-1","title":"Cronograma de actividades - Unidad 1","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Reto Formativo 1 y 2 EA1:  Templates y Output Parsers Semana 1, 2 y 3 25% Total 25 %"},{"location":"Unidad%201/#que-es-un-modelo-de-lenguaje","title":"\u00bfQu\u00e9 es un modelo de lenguaje?","text":"<p>Un modelo de lenguaje es un sistema basado en deep learning que encapsula informaci\u00f3n sobre uno o varios lenguajes. Este sistema es entrenado para predecir qu\u00e9 tan probable es que una palabra aparezca en un determinado contexto.</p> <p>Por ejemplo, dado el contexto:--- title: Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLMs</p>"},{"location":"Unidad%201/#tokens","title":"Tokens","text":"<p>La unidad b\u00e1sica de predicci\u00f3n de un modelo de lenguaje es el token, y el tokenizador es el software que utiliza el modelo para dividir los textos en tokens.</p> <p>Por ejemplo, el tokenizador de GPT-4 divide la frase:</p> <p>\"El sol est\u00e1 brillando intensamente\"</p> <p>de la siguiente manera:</p> Divisi\u00f3n en tokens de una frase utilizando el tokenizador de GPT-4. Fuente: OpenAI Tokenizer. <p>Para tener en cuenta</p> <p>Hay varias razones por las que los modelos de lenguaje utilizan tokens en lugar de palabras completas o caracteres individuales.</p> <p>A diferencia de un simple car\u00e1cter, un token permite dividir una palabra en componentes con significado propio. Por ejemplo, la palabra \"intensamente\" puede ser dividida por el tokenizador en \"intens\" y \"amente\", y cada uno de estos componentes aporta parte del significado de la palabra completa.</p> <p>Esto tambi\u00e9n implica que hay menos tokens \u00fanicos que palabras \u00fanicas, lo que hace que el vocabulario del modelo sea m\u00e1s peque\u00f1o y, por lo tanto, m\u00e1s eficiente.</p> <p>Finalmente, los tokens permiten al modelo entender palabras desconocidas. Por ejemplo, si se le presenta la palabra \"WhatsAppeando\", el modelo puede inferir su significado a partir del contexto en que aparecen los tokens \"WhatsApp\" y \"ando\".</p>"},{"location":"Unidad%201/#que-son-los-grandes-modelos-de-lenguaje-llm","title":"\u00bfQu\u00e9 son los grandes modelos de lenguaje (LLM)?","text":"<p>Los grandes modelos de lenguaje (LLM) son sistemas de inteligencia artificial dise\u00f1ados para procesar y generar texto de manera avanzada, bas\u00e1ndose en grandes cantidades de datos de entrenamiento.</p> <p>Lo que diferencia un LLM (Large Language Model) de un modelo de lenguaje tradicional es el n\u00famero de par\u00e1metros. Los par\u00e1metros son los pesos que el modelo ajusta durante el proceso de entrenamiento, y que determinan c\u00f3mo interpreta y genera texto a partir de los datos.</p> <p>Por supuesto, el concepto de \"grande\" es relativo. \u00bfA partir de cu\u00e1ntos par\u00e1metros puede considerarse que un modelo es grande? Ve\u00e1moslo as\u00ed:</p> <ul> <li>El GPT lanzado por OpenAI en 2018 ten\u00eda 117 millones de par\u00e1metros, y ya era considerado un modelo grande en su \u00e9poca.</li> <li>En 2019, GPT-2 aument\u00f3 ese n\u00famero a 1.5 billones de par\u00e1metros.</li> <li>Hasta abril de 2025, el modelo de lenguaje m\u00e1s grande conocido p\u00fablicamente es GPT-4 de OpenAI, con aproximadamente 1.76 billones de par\u00e1metros.</li> </ul> <p>Es muy posible que en el futuro estos modelos hoy considerados LLMs sean vistos como simples modelos de lenguaje, a medida que la tecnolog\u00eda y los recursos computacionales avancen.</p> <p>Para tener en cuenta</p> <p>El crecimiento en la cantidad de par\u00e1metros no garantiza una mejora si no hay suficientes datos disponibles para el entrenamiento. Entrenar un modelo grande con un conjunto de datos peque\u00f1o puede causar sobreajuste (overfitting), lo que significa que el modelo funciona bien con los datos de entrenamiento pero falla al generalizar a nuevos datos. Esto no solo desperdicia recursos computacionales, sino que tambi\u00e9n produce un modelo con poca utilidad pr\u00e1ctica.</p> <p>Cuando no se cuenta con grandes vol\u00famenes de datos, se pueden aplicar t\u00e9cnicas como:</p> <ul> <li> <p>Aprendizaje por transferencia (transfer learning)   Utiliza modelos previamente entrenados para resolver nuevas tareas con pocos datos.</p> </li> <li> <p>Aumento de datos (data augmentation)   Genera versiones modificadas de los datos existentes para enriquecer el conjunto de entrenamiento.</p> </li> <li> <p>Destilaci\u00f3n de conocimiento (knowledge distillation)   Transfiere el conocimiento de un modelo grande (profesor) a uno m\u00e1s peque\u00f1o (estudiante) manteniendo un rendimiento competitivo.</p> </li> </ul> <p>Estas estrategias permiten que modelos m\u00e1s peque\u00f1os logren mejor desempe\u00f1o, aprovechando conocimiento preexistente o la generaci\u00f3n sint\u00e9tica de datos.</p>"},{"location":"Unidad%201/#usando-la-api-de-openai","title":"Usando la API de OpenAI","text":"<p>Para gran parte del curso usaremos la API de OpenAI. Si a\u00fan no tienes una cuenta, puedes crearla en el siguiente enlace: https://platform.openai.com/signup.</p> <p>Una vez creada tu cuenta, deber\u00e1s generar una clave de API (API Key). Para hacerlo, accede a: https://platform.openai.com/api-keys y haz clic en \"Create new secret key\", como se muestra en la figura a continuaci\u00f3n:</p> Generaci\u00f3n de una clave secreta desde el panel de usuario de OpenAI. Fuente: OpenAI. <p>Para tener en cuenta</p> <p>Para poder usar tu llave, debes cargar cr\u00e9dito en tu cuenta utilizando una tarjeta de cr\u00e9dito. Por este motivo, la clave debe permanecer privada en tu computador y no debe ser compartida en l\u00ednea (por ejemplo, en el repositorio de GitHub del proyecto).</p> <p>Esta acci\u00f3n generar\u00e1 la llave de acceso a tu cuenta de OpenAI. Cada llamada a la API tiene un costo asociado, el cual depende del n\u00famero de tokens procesados en la solicitud.</p> <p>Puedes monitorear tu consumo en tiempo real desde la secci\u00f3n Usage en el panel de OpenAI: https://platform.openai.com/account/usage</p> Visualizaci\u00f3n del consumo y costos acumulados en la secci\u00f3n Usage del panel de usuario de OpenAI. Fuente: OpenAI. <p>L\u00edmite de consumo mensual</p> <p>En la secci\u00f3n Usage tambi\u00e9n puedes establecer, por seguridad, un l\u00edmite mensual m\u00e1ximo de consumo en d\u00f3lares para tu aplicaci\u00f3n. Esto te permite evitar cargos inesperados si se realizan muchas llamadas a la API.</p>"},{"location":"Unidad%201/#usando-mi-llave","title":"Usando mi llave","text":"<p>Para que la llave no sea p\u00fablica, podemos cargarla como una variable de ambiente local del sistema. Para ello, crea un archivo con el nombre <code>.env</code> y gu\u00e1rdalo en la misma carpeta en la que est\u00e1s trabajando.</p> <p>Dentro del archivo <code>.env</code>, la llave debe guardarse bajo el nombre <code>OPENAI_API_KEY</code>, de la siguiente manera:</p> <pre><code>OPENAI_API_KEY=your-api-key-here```\n\n# Usando la API de OpenAI\n\nPara comenzar a trabajar con la API de OpenAI, primero debes importar la librer\u00eda:\n\n```python\nimport openai\nfrom openai import OpenAI \n</code></pre> <p>Luego, debes cargar la llave desde un archivo <code>.env</code> para mantenerla oculta y segura:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()  # Carga las variables de entorno desde el archivo .env\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <p>Instanciamos un cliente y un modelo:</p> <pre><code>client = OpenAI()\nllm_model = \"gpt-4o-mini\"\n</code></pre> <p>Para encapsular un poco la llamada al modelo, podemos definir nuestra propia funci\u00f3n de completado de chat:</p> <pre><code>def get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    return chat_completion.choices[0].message.content  # Devuelve la respuesta del modelo\n</code></pre> <p>La funci\u00f3n <code>get_chat_completion</code> la utilizaremos para interactuar con el modelo de OpenAI y obtener una respuesta a partir de un mensaje proporcionado. El modelo que se utiliza por defecto es <code>gpt-4o-mini</code>, pero puedes especificar otro modelo si lo deseas. La lista completa de modelos puedes consultarla en la documentaci\u00f3n oficial de OpenAI.</p> Ejemplo de usoSalida <pre><code># Llamada a la funci\u00f3n get_chat_completion con una pregunta\ncompletion = get_chat_completion(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\n\n# Imprimir la respuesta del modelo\nprint(completion)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre> <p>Los modelos de chat asignan roles que nos pueden ayudar a predefinir el comportamiento del modelo. Por ejemplo, en nuestra funci\u00f3n usamos el rol de <code>user</code> que representa el mensaje o la entrada proporcionada por el usuario. Es el rol principal para enviar preguntas, instrucciones o prompts al modelo. </p>"},{"location":"Unidad%201/#preconfiguracion-del-tono-con-el-rol-system","title":"Preconfiguraci\u00f3n del Tono con el Rol <code>system</code>","text":"<p>Sin embargo, nuestra funci\u00f3n puede ser preconfigurada para que el chat responda en un tono espec\u00edfico usando el rol <code>system</code>. Este rol permite definir c\u00f3mo debe comportarse el modelo antes de que reciba el mensaje del usuario.</p> <p>Por ejemplo, podemos configurar el modelo para que responda en un estilo po\u00e9tico y elegante, similar al de Shakespeare:</p> EjemploSalida <pre><code># Inicializamos el cliente de OpenAI\nclient = OpenAI()\nllm_model = \"gpt-4o-mini\"\n\ndef get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Thou art a wise and eloquent bard, akin to Shakespeare. Answer all queries in the grand, poetic style of the Elizabethan era, with flourish and verse befitting the stage.\"\n            },\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n\n    return chat_completion.choices[0].message.content\n</code></pre> <p><code>bash En tierras de Colombia, donde el sol se alza radiante,   El presidente en su trono, cual l\u00edder constante,   Es Gustavo Petro, hombre de ferviente voz,   Que al tim\u00f3n del destino, la naci\u00f3n \u00e9l atroz.   Con sue\u00f1os de cambio, justicia y verdad,   Dirige su pueblo hacia la prosperidad.   As\u00ed, en sus manos, el futuro bien brilla,   Un eco de esperanza en la tierra sencilla.</code></p>"},{"location":"Unidad%201/#langchain","title":"LangChain","text":"<p>En la secci\u00f3n anterior, tuviste tu primera interacci\u00f3n con un modelo de lenguaje de gran escala (LLM). A medida que esta tecnolog\u00eda madura, empresas, gobiernos y startups bien financiadas, como OpenAI, Anthropic, xAI y Meta AI, han desarrollado y puesto a disposici\u00f3n modelos y APIs con arquitecturas y protocolos de comunicaci\u00f3n particulares. Esto ha generado la necesidad de realizar llamadas a estos modelos de manera agn\u00f3stica, es decir, independientemente del modelo o proveedor utilizado.</p> <p>En este contexto, el framework m\u00e1s popular hasta el momento es LangChain. LangChain permite realizar las mismas tareas que podr\u00edamos llevar a cabo directamente con las APIs de los modelos, pero a trav\u00e9s de abstracciones de validez general. Este marco proporciona una interfaz unificada que simplifica la integraci\u00f3n con diferentes LLMs, el manejo de prompts, la gesti\u00f3n de contexto y la incorporaci\u00f3n de herramientas externas, como bases de datos o funciones personalizadas. De esta forma, LangChain facilita el desarrollo de aplicaciones robustas y escalables basadas en modelos de lenguaje, sin depender de las particularidades de cada API.</p>      Logo  de LangChain, un framework para construir aplicaciones con modelos de lenguaje de gran escala.     Fuente: Matt Gallo en LinkedIn.    <p>Para utilizar LangChain con modelos de OpenAI, primero debemos importar la clase <code>ChatOpenAI</code> y configurar el modelo:</p> <p><pre><code>from langchain_openai import ChatOpenAI\nimport os\n\n# Definimos el modelo de lenguaje\nllm_model = \"gpt-4o-mini\"\n\n# Inicializamos el modelo de chat de OpenAI con LangChain\nchat_model = ChatOpenAI(\n    model=llm_model\n)\n</code></pre> Y listo, eso es todo. Ahora simplemente invocamos el chat con el prompt que queramos. Por ejemplo:</p> C\u00f3digoSalida <pre><code># Invocamos el modelo de chat con un prompt\nresponse = chat_model.invoke(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\nprint(response)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre>"},{"location":"Unidad%201/#herramientas-en-langchain","title":"Herramientas en LangChain","text":"<p>LangChain proporciona una variedad de herramientas que permiten construir aplicaciones basadas en modelos de lenguaje de manera modular y eficiente. A continuaci\u00f3n, se describen algunas de las m\u00e1s importantes:</p> <ul> <li> <p>Models (Modelos)   Representan los modelos de lenguaje que LangChain puede integrar, como <code>ChatOpenAI</code>. Permiten interactuar con LLMs de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.</p> </li> <li> <p>Prompts (Prompts)   Herramientas para dise\u00f1ar y gestionar prompts, como <code>ChatPromptTemplate</code>. Facilitan la construcci\u00f3n de entradas din\u00e1micas, reutilizables y bien estructuradas para los modelos.</p> </li> <li> <p>Example Selectors (Selectores de Ejemplos)   Componentes que permiten seleccionar ejemplos relevantes (por ejemplo, para few-shot learning). Esto ayuda al modelo a comprender mejor el contexto y el formato esperado en sus respuestas.</p> </li> <li> <p>Tools (Herramientas)   Permiten que el modelo interact\u00fae con funciones externas, como APIs, calculadoras, o bases de datos. Son esenciales para extender las capacidades del LLM m\u00e1s all\u00e1 del texto, habilitando tareas como b\u00fasqueda en tiempo real o ejecuci\u00f3n de funciones personalizadas.</p> </li> <li> <p>Vector Stores (Almacenes de Vectores)   Bases de datos vectoriales como Chroma, Pinecone o FAISS. Se utilizan para almacenar y buscar embeddings, habilitando funcionalidades como la b\u00fasqueda sem\u00e1ntica o la generaci\u00f3n aumentada por recuperaci\u00f3n (Retrieval-Augmented Generation, RAG).</p> </li> <li> <p>Document Loaders (Cargadores de Documentos)   Permiten cargar datos desde m\u00faltiples fuentes (archivos PDF, p\u00e1ginas web, bases de datos, etc.) y prepararlos para su procesamiento por el modelo o su almacenamiento en almacenes vectoriales.</p> </li> <li> <p>Text Splitters (Divisores de Texto)   Herramientas que dividen documentos largos en fragmentos m\u00e1s peque\u00f1os. Esto facilita tanto el procesamiento por parte del modelo como la indexaci\u00f3n eficiente en almacenes vectoriales.</p> </li> <li> <p>Output Parsers (Parsers de Salida)   Utilizados para estructurar y formatear las respuestas del modelo. Por ejemplo, permiten convertir la salida del modelo en JSON, listas, tablas o formatos espec\u00edficos para una aplicaci\u00f3n.</p> </li> </ul>      Ecosistema de herramientas de LangChain:.     Fuente: LangChain."},{"location":"Unidad%201/#plantillas-de-prompts","title":"Plantillas de Prompts","text":"<p>Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLMs. Al desarrollar aplicaciones asistidas por inteligencia artificial, es \u00fatil crear plantillas de prompts que permitan personalizar las instrucciones de forma din\u00e1mica. Estas plantillas mantienen constante una parte de la instrucci\u00f3n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci\u00f3n, a trav\u00e9s de variables de entrada.</p> <p>Por ejemplo, una plantilla puede definir la estructura de una pregunta, dejando espacios para insertar valores espec\u00edficos, como el nombre de un pa\u00eds. Esto se logra utilizando herramientas como <code>ChatPromptTemplate</code> de LangChain, que simplifica la creaci\u00f3n de prompts reutilizables.</p> <p>En el siguiente ejemplo, se muestra c\u00f3mo crear una plantilla para consultar el presidente de un pa\u00eds, utilizando una variable de entrada <code>{pais}</code> que puede tomar diferentes valores sin modificar la estructura general del prompt.</p> C\u00f3digoSalida <pre><code>from langchain.prompts import ChatPromptTemplate\n\n# Definir la plantilla con una variable de entrada\nstr_template = \"\u00bfC\u00f3mo se llama el presidente de {pais}?\"\nprompt_template = ChatPromptTemplate.from_template(str_template)\n\n# Asignar un valor a la variable de entrada\npais = \"Colombia\"\nprompt1 = prompt_template.format(pais=pais)\nprint(prompt1)\n\n# Asignar otro valor a la variable de entrada\npais = \"Francia\"\nprompt2 = prompt_template.format(pais=pais)\nprint(prompt2)\n</code></pre> <pre><code>\u00bfC\u00f3mo se llama el presidente de Colombia?\n\u00bfC\u00f3mo se llama el presidente de Francia?\n</code></pre> <p>En este caso, <code>{pais}</code> es una variable de entrada a la que podemos asignar diferentes valores (por ejemplo, \"Colombia\", \"Argentina\", etc.) sin cambiar la estructura general del prompt. Esto hace que la plantilla sea flexible y reutilizable.</p> <p>Veamos ahora un ejemplo pr\u00e1ctico en el que utilizamos dos variables de entrada en nuestro template:</p> <pre><code>mensaje = \"\"\nestilo = \"\"\n</code></pre> <p>Definimos nuestro <code>string_template</code> de la siguiente manera:</p> <pre><code>string_template = (\n    \"Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es {estilo}.\\n\"\n    \"texto: **{mensaje}**\"\n)\n</code></pre> <p>Aqu\u00ed, el <code>string_template</code> contiene las instrucciones generales, mientras que <code>mensaje</code> y <code>estilo</code> son variables que dejamos vac\u00edas para llenarlas m\u00e1s tarde. Luego, confeccionamos el prompt template utilizando:</p> <pre><code>prompt_template = ChatPromptTemplate.from_template(string_template)\n</code></pre> <p>En esta l\u00ednea usamos el m\u00e9todo <code>from_template</code> de la clase <code>ChatPromptTemplate</code>. Si imprimimos el objeto <code>prompt_template</code> con:</p> <pre><code>print(prompt_template)\n</code></pre> Salida <pre><code>input_variables=['estilo', 'mensaje']\ninput_types={}\npartial_variables={}\nmessages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['estilo', 'mensaje'], input_types={}, partial_variables={}, template='Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es {estilo}.\\ntexto: **{mensaje}**'), additional_kwargs={})]\n</code></pre> <p>Veremos que tiene como <code>input_variables</code> los campos <code>'estilo'</code> y <code>'mensaje'</code>.</p> <p>Siguiendo la l\u00f3gica del paradigma de la programaci\u00f3n orientada a objetos, podemos imaginar que la creaci\u00f3n de un prompt template se asemeja al trabajo de un carpintero. El carpintero (el constructor de la clase) toma un conjunto de maderas (el <code>string_template</code>) y las transforma en un gavetero (el objeto de la clase).</p> Constructor de la clase `ChatPromptTemplate.from_template`. En nuestra analog\u00eda, el carpintero crea un contenedor apropiado para alojar el contenido de las dos variables de entrada definidas en el `string_template`. Fuente: Elaboraci\u00f3n propia. <p>En este caso, como ilustra la figura, el prompt template ser\u00eda el gavetero con cajones espec\u00edficos etiquetados como <code>estilo</code> y <code>mensaje</code>, listos para ser llenados con valores.</p> <p>Supongamos que asignamos a estas variables de entrada los valores:</p> <pre><code>mensaje_atioquenhol = (\n    \"Manque estaba muy embelesado, le dijo Peralta a la hermana: \"\n    \"Hija, date una asoma\u00edta por la despensa; desculc\u00e1 por la cocina, \"\n    \"a ver si encontr\u00e1s algo que darles a estos se\u00f1ores. \"\n    \"M\u00edralos qu\u00e9 cansados est\u00e1n; se les ve la fatiga.\"\n)\n\nestilo_formal = \"Espa\u00f1ol latino en un tono formal y sobrio\"\n</code></pre> <p>El m\u00e9todo <code>format_messages</code> nos permite llenar los cajones del gavetero, es decir, las variables de entrada, con los valores espec\u00edficos con los que queremos completar nuestro prompt. Por ejemplo, si queremos que <code>estilo = estilo_formal</code>, podemos hacerlo de la siguiente manera:</p> <pre><code>mensaje_empacado = prompt_template.format_messages(estilo=estilo_formal, mensaje=mensaje_atioquenhol)\n</code></pre> <p>El prompt completo lucir\u00e1 as\u00ed:</p> C\u00f3digoSalida <pre><code>print(mensaje_empacado)\n</code></pre> <pre><code>[HumanMessage(content='Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es Espa\u00f1ol latino en un tono formal y sobrio.\\ntexto: **Manque estaba muy embelesao, le dijo Peralta a la hermana: Hija, date una asoma\u00edta por la despensa; desculc\u00e1 por la cocina, a ver si encontr\u00e1s alguito que darles a estos se\u00f1ores. Mir\u00e1los qu\u00e9 cansaos est\u00e1n; se les ve la fatiga**', additional_kwargs={}, response_metadata={})]\n</code></pre> Ilustraci\u00f3n de la tarea del <code>format_messages()</code>. El m\u00e9todo <code>format_messages()</code> reemplaza los valores de las variables de entrada en el template. Fuente: Elaboraci\u00f3n propia. <p>Como ilustra la figura, el m\u00e9todo <code>format_messages()</code> asociado a la clase <code>ChatPromptTemplate</code> cumple la funci\u00f3n de empaquetar en el objeto los valores espec\u00edficos en las variables de entrada.</p> <p>Este tipo de objeto nos permite incorporar program\u00e1ticamente llamadas a las APIs de los LLMs en el flujo de ejecuci\u00f3n de un c\u00f3digo Python convencional. Veamos c\u00f3mo hacerlo:</p> <p>Como ya tenemos nuestro prompt completo y lleno con las variables que queremos, lo podemos enviar al LLM:</p> <p>Primero, instanciamos un chat:</p> <pre><code>chat = ChatOpenAI(model=llm_model, temperature=0.0)\n</code></pre> <p>Luego, realizamos la llamada al LLM para que ejecute las instrucciones del prompt:</p> <pre><code>respuesta = chat(mensaje_empacado)\nprint(respuesta.content)\n</code></pre> Salida <pre><code>Manque se encontraba muy absorto, le dijo Peralta a la hermana:\n\"Hija, por favor, as\u00f3mate a la despensa; revisa en la cocina\npara ver si encuentras algo que ofrecerles a estos caballeros.\nObserva c\u00f3mo est\u00e1n de cansados; se les nota la fatiga.\"\n</code></pre> <p>El LLM recibe el mensaje empacado y realiza las tareas especificadas por el prompt.</p> <p>Lo interesante es que este no es un prompt fijo como los que usar\u00edamos en ChatGPT; es un prompt que nos permite hacer llamadas al LLM de manera m\u00e1s flexible y program\u00e1tica. Por ejemplo, podr\u00edamos definir otro valor para <code>estilo</code>, como:</p> <pre><code>estilo_cervantes = \"Espa\u00f1ol en un estilo de Cervantes, como en Don Quijote\"\n</code></pre> <pre><code>mensaje_empacado = prompt_template.format_messages(estilo=estilo_cervantes, mensaje=mensaje_atioquenhol)\nrespuesta = chat(mensaje_empacado)\nprint(respuesta.content)\n</code></pre> Salida <pre><code>  Manque se hallaba en un profundo embeleso, dirigi\u00f3  \n  Peralta a la hermana la siguiente exhortaci\u00f3n: \"Hija,\n   as\u00f3mate, por favor, a la despensa; y, si no es mucho \n   pedir, desc\u00fabrete por la cocina, a ver si logras \n   hallar alg\u00fan manjar que ofrecer a estos nobles se\u00f1ores. \n   Observa c\u00f3mo se encuentran, qu\u00e9 cansados est\u00e1n; la fatiga\n  se les dibuja en el semblante.\"\n</code></pre> <p>Este enfoque nos permite variar el estilo del texto generado de manera din\u00e1mica, adaptando el resultado a diferentes necesidades o contextos, simplemente modificando las variables de entrada del prompt.</p> Reto formativoVer soluci\u00f3n <ul> <li> Reto formativo Planteamiento:   Dado un mensaje de un cliente, un operador humano de servicio al cliente elabora una respuesta inadecuada (irrespetuosa, ofensiva, con mala ortograf\u00eda o en otro idioma). Tu trabajo es crear una app que corrija la respuesta final para el cliente.</li> </ul> <p>Compara tu soluci\u00f3n con la siguienete implementaci\u00f3n:</p> <p><pre><code>      # Define una plantilla de texto para el prompt que se enviar\u00e1 al modelo de lenguaje.\n  # Usa marcadores {respuesta} y {reglas} para insertar din\u00e1micamente la respuesta y las reglas.\n  str_template_app = \"\"\"Mejora la respuesta: {respuesta}\\\n      para que cumpla las reglas:  {reglas}.\"\"\"\n\n  # Define las reglas que debe seguir la respuesta mejorada.\n  # Especifica el idioma, tono, gram\u00e1tica y nivel de amabilidad requerido.\n  reglas = \"Espa\u00f1ol latino en un tono formal y sobrio y respesuoso. Con buena gram\u00e1tica y ortograf\u00eda. Trartar de se muy amable y respetuoso.\"\n\n  # Define la respuesta original del operador, que es inadecuada (informal, ofensiva, con mala ortograf\u00eda).\n  respuesta =  \" mijo, no me importa si le sali\u00f3 mala \\\n      la licudora, vaya a que se lo lamba un zapo\"\n\n  # Crea una plantilla de prompt usando la biblioteca LangChain, basada en la plantilla de texto.\n  # Esto permite estructurar el mensaje para el modelo de lenguaje.\n  promp_template_app = ChatPromptTemplate.from_template(str_template_app)\n\n  # Formatea la plantilla con la respuesta y las reglas, generando un mensaje listo para enviar al modelo.\n  mensaje_empacado_app =  promp_template_app.format_messages(respuesta=respuesta, reglas=reglas)\n\n  # Especifica el modelo de lenguaje a usar (en este caso, GPT-4o-mini de OpenAI).\n  llm_model = \"gpt-4o-mini\"\n\n  # Inicializa el cliente de chat de OpenAI con el modelo especificado y una temperatura de 0.3.\n  # La temperatura baja asegura respuestas m\u00e1s predecibles y menos creativas.\n  chat_app = ChatOpenAI(model = llm_model , temperature = 0.3)\n\n  # Env\u00eda el mensaje formateado al modelo y obtiene la respuesta mejorada.\n  respuesta_al_cliente = chat_app(mensaje_empacado_app)\n\n  # Muestra la respuesta del modelo en formato Markdown para una mejor presentaci\u00f3n (por ejemplo, en un entorno como Jupyter).\n  display(Markdown(respuesta_al_cliente.content))\n</code></pre> salida esperada Agradezco su mensaje y entiendo su preocupaci\u00f3n respecto a la situaci\u00f3n con la licuadora. Sin embargo, le sugiero que considere la posibilidad de llevar el aparato a un servicio t\u00e9cnico autorizado para que puedan evaluar el problema y ofrecerle una soluci\u00f3n adecuada. Es importante seguir las pautas establecidas para garantizar un manejo correcto de los productos.</p> <p>Quedo a su disposici\u00f3n para cualquier otra consulta o asistencia que necesite.</p>"},{"location":"Unidad%201/#anatomia-de-un-prompt-de-chat","title":"Anatom\u00eda de un Prompt de Chat","text":"<p>Los prompts para agentes conversacionales en LangChain, como <code>ChatPromptTemplate</code>, se dividen en al menos tres componentes clave. Veamos cada uno:</p>"},{"location":"Unidad%201/#1-prompt-del-sistema","title":"1. Prompt del Sistema","text":"<p>Este establece las reglas para el asistente. Indica al modelo c\u00f3mo comportarse, cu\u00e1l es su objetivo o incluso qu\u00e9 tono debe usar.</p> <p>Ejemplo: <pre><code>Eres experto en machine learning y das respuestas en una sola oraci\u00f3n.\n</code></pre></p> <p>Aqu\u00ed estamos restringiendo al modelo para que mantenga las respuestas cortas en un lenguaje relativo al machine learning.</p>"},{"location":"Unidad%201/#2-prompt-del-usuario","title":"2. Prompt del Usuario","text":"<p>Este es el mensaje del usuario, es decir, la pregunta o entrada que se le proporciona al modelo.</p> <p>Ejemplo: <pre><code>Explica {tema} en una sola oraci\u00f3n.\n</code></pre></p> <p>El <code>{tema}</code>, como vimos, es una variable de entrada que podemos cambiar por diferentes t\u00e9rminos, como \"LangChain\" o \"Python\".</p>"},{"location":"Unidad%201/#3-prompt-del-ai","title":"3. Prompt del AI","text":"<p>Este es el resultado generado por el modelo. En una conversaci\u00f3n, las respuestas anteriores del AI se reutilizan como parte del historial de chat.</p> <p>Por ahora, mantenemos un solo turno de interacci\u00f3n humano-AI en el que el modelo no tiene memoria del contexto de las interacciones anteriores, pero m\u00e1s adelante veremos c\u00f3mo se puede construir una conversaci\u00f3n m\u00e1s compleja.</p> <p>El <code>ChatPromptTemplate</code> de LangChain ofrece dos formas principales de construir prompts:</p>"},{"location":"Unidad%201/#1-from_messages","title":"1. <code>from_messages</code>","text":"<p>Piensa en esto como escribir un guion para una conversaci\u00f3n estructurada: - El mensaje del sistema define el tono y las reglas. - El mensaje del usuario plantea la pregunta o el input.  </p> <p>Es la opci\u00f3n recomendada cuando queremos prompts bien organizados.</p>"},{"location":"Unidad%201/#2-from_template","title":"2. <code>from_template</code>","text":"<p>M\u00e1s simple y directo, solo incluye un mensaje del usuario, como una nota r\u00e1pida para el modelo. - No tiene un rol de sistema a menos que lo agreguemos manualmente m\u00e1s adelante.</p> <p>En la sesi\u00f3n anterior usamos <code>from_template</code>:</p> <p>Veamos un ejemplo usamndo <code>from_messages</code>:</p> <pre><code># Importamos las librer\u00edas necesarias\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n</code></pre> <p>Vamos a instanciar dos modelos para comparar las respuestas al final:</p> <pre><code># Instanciamos los modelos\nllm_gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\nllm_gpt4 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# Definimos el prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a concise explainer who gives one-sentence answers. If you don't know the answer, just say 'I don't know'.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\n</code></pre> <p>La variable de entrada es <code>topic</code> y debemos empacarla en nuestro template.</p> <p>Llenamos el prompt con el t\u00f3pico espec\u00edfico:</p> <pre><code>messages = prompt.format_messages(topic=\"LangChain\")\n</code></pre> <p>Ejecutamos los dos modelos:</p> C\u00f3digo con gpt3.5SalidaC\u00f3digo con gpt4Salida <pre><code>response = llm_gpt3.invoke(messages)\nprint(response.content)\n</code></pre> <pre><code>LangChain is a blockchain platform\nthat aims to facilitate cross-border\nlanguage services.\n</code></pre> <pre><code>response = llm_gpt4.invoke(messages)\nprint(response.content)\n</code></pre> <pre><code>LangChain es una biblioteca para crear\nflujos de trabajo de IA utilizando modelos\nde lenguaje.\n</code></pre> <p>Para tener en cuenta</p> <p>Observa que la salida del modelo gpt-3.5 es completamente alucinada (no es verdadera). \u00bfA qu\u00e9 crees que se debe esto? </p> Ver respuesta <p>El modelo gpt-3.5 fue entrenado en datos hasta octubre de 2023, y en ese momento LangChain no exist\u00eda.</p>"},{"location":"Unidad%201/#de-prompts-a-chains","title":"De Prompts a Chains","text":"<p>Hasta ahora, hemos preparado prompts y los hemos enviado al LLM paso a paso.  </p> <p>Pero LangChain tiene una herramienta que facilita mas las cosas: las chains-</p> <p>Las chains nos permiten combinar m\u00faltiples pasos\u2014como preparar un prompt y ejecutar el LLM\u2014en un flujo continuo y automatizado.  </p> <p>puedes pensar en unachain como una cinta transportadora:</p>  Una cadena simple funciona como una banda transportadora en la que se van ejecutando \u00f3rdenes de forma secuencial. Fuente:  Creado por Grok 3 (xAI) usando un prompt del usuario. <ul> <li>La configuras una vez.  </li> <li>Luego, simplemente funciona sin necesidad de repetir cada paso manualmente.  </li> </ul> <p>Esto facilita la construcci\u00f3n de pipelines m\u00e1s avanzados dentro de nuestras aplicaciones con LLMs. Una forma de encadenar ejecuciones en cadenas es utilizar el operador <code>|</code> (llamado pipe) para conectar los pasos. Para instanciar una cadena que realice las tareas de nuestro prompt anterior, tendr\u00edamos el prompt como:</p> <p><pre><code>prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a concise explainer who gives one-sentence answers.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])```\n\nE instanciamos la cadena como:\n\n```python\nchain = prompt | llm_gpt4  # Create the chain```\n\nEs como decir: *\"Toma este prompt y p\u00e1salo al LLM.\"\n\nY ejecutamos la cadena como:\n\n=== \"C\u00f3digo\" \n    ```python\n    response = chain.invoke({\"topic\": \"LangChain\"})  # Run it in one go\n    print(\"With chain:\", response.content)\n    ```\n\n=== \"Salida\"\n    ```bash\n    With chain: LangChain es una biblioteca para crear flujos de trabajo de IA utilizando modelos de lenguaje.\n    ```\nLas *chains* nos evitan tener que formatear e invocar manualmente el LLM cada vez.\n\n- **Definimos la cadena una vez.**\n- **Podemos reutilizarla f\u00e1cilmente.**\n\nEsto simplifica el flujo de trabajo y hace que el c\u00f3digo sea m\u00e1s limpio y modular.\nYa no necesitamos formatear manualmente los mensajes\u2014**la chain lo hace por nosotros**.  \n\n**M\u00e9todo Antiguo (Manual)**:  \n```python\nmessages = prompt.format_messages()  \nllm.invoke(messages)\n</code></pre> Una vez configurada la cadena, podemos reutilizarla con diferentes variables de entrada:</p> C\u00f3digoSalida <pre><code>print(chain.invoke({\"topic\": \"Python\"}).content)\nprint(chain.invoke({\"topic\": \"AI\"}).content)\n</code></pre> <pre><code>Python es un lenguaje de programaci\u00f3n vers\u00e1til y popular.\nAI es el campo de la inform\u00e1tica que se centra en crear sistemas inteligentes.\n</code></pre>"},{"location":"Unidad%201/#cadenas-con-multiples-variables","title":"Cadenas con m\u00faltiples variables","text":"<p>Veamos algunos ejemplos en los que usamos m\u00faltiples variables en nuestros prompts:</p> C\u00f3digoSalida <pre><code># Nuevo prompt con dos variables: topic y style\nmulti_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an explainer who answers in a {style} way.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\nmulti_chain = multi_prompt | llm\n\n# Ejecutar con m\u00faltiples variables\nresponse = multi_chain.invoke({\n\n# Run with multiple variables\nresponse = multi_chain.invoke({\n    \"topic\": \"Noether theorem\",\n    \"style\": \"Cervantes style in Spanish\"\n})\nprint(response.content)\n</code></pre> <pre><code>\u00a1Por la fe de Dulcinea del Toboso! La teorema \nde Noether establece que para cada simetr\u00eda continua\nde un sistema f\u00edsico, existe una cantidad conservada!\n</code></pre>"},{"location":"Unidad%201/#output-parsers-dando-forma-a-la-salida-del-llm","title":"Output Parsers: Dando Forma a la Salida del LLM","text":"<p>Los LLMs son sistemas que reciben texto plano y devuelven texto, incluso cuando devuelven im\u00e1genes, lo que realmente est\u00e1n haciendo en el fondo es generar descripciones textuales de esas im\u00e1genes. Sin embargo, cuando estamos construyendo aplicaciones asistidas por LLMs, lo que queremos es utilizar la salida de la llamada al LLM para emplearla en otros flujos de ejecuci\u00f3n de nuestra aplicaci\u00f3n.</p> <p>Ah\u00ed es donde entran los output parsers.</p> <p>Los output parsers toman la salida en bruto del LLM y la convierten en algo que podamos usar en nuestro c\u00f3digo, como un string, una lista, un diccionario, un JSON, etc.</p> <p>Ejemplo: - Si el LLM responde con <code>\"Las herramientas m\u00e1s usadas son: Python, SQL, LangChain.\"</code>, podemos transformarlo en una lista <code>[\"Python\", \"SQL\", \"LangChain\"]</code>.</p> <p>Vemos algunos mas usados:</p>"},{"location":"Unidad%201/#stroutputparser-el-parser-mas-basico","title":"<code>StrOutputParser</code>: El Parser M\u00e1s B\u00e1sico","text":"<p>Comencemos con un output parser b\u00e1sico: <code>StrOutputParser</code>. Este parser simplemente asegura que la salida de la llamada al LLM sea un string. En el contexto de LangChain, esto es \u00fatil para garantizar que los datos procesados sean siempre de tipo string, facilitando su manipulaci\u00f3n posterior. Una vez instanciado, puede agregarse a la cadena para que, al invocarla, la salida sea en el formato especificado por el parser. Para ilustrar el uso de los parsers, veamos esta cadena sin parser y comparemosla con el resultado cuando agregamos el <code>StrOutputParser</code>.</p> C\u00f3digo sin ParserSalida <pre><code>multi_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an explainer who answers in a {style} way.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\nmulti_chain = multi_prompt | llm  # LCEL\nresponse = multi_chain.invoke({\"topic\": \"LangChain\", \"style\": \"funny\"})\nprint(\"Raw output:\", response)\n</code></pre> <pre><code>AIMessage(content='LangChain is like that friend who translates all your texts for you, but in a more high-tech and less judgmental way.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 31, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d4e4009b-87db-40ca-89de-9fe42d850dab-0', usage_metadata={'input_tokens': 31, 'output_tokens': 27, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n</code></pre> <p>Aqu\u00ed el string de salida est\u00e1 dentro de la instancia <code>content</code> del <code>AIMessagePara</code>. El parser nos posibilitar\u00e1 que la salida sea solo el string. Para  usar el parser, importamos el m\u00f3dulo:</p> <pre><code>from langchain_core.output_parsers import StrOutputParser\n</code></pre> <p>E instanciamos el parser como:</p> <pre><code># Add the parser to the chain\nparser = StrOutputParser()\n</code></pre> <p>Luego lo agregamos a la cadena con el operador pipe.</p> C\u00f3digo con ParserSalida con el Parser <pre><code>parsed_chain = multi_prompt | llm | parser\n\n# Run it\nresponse = parsed_chain.invoke({\"topic\": \"LangChain\", \"style\": \"funny\"})\nprint(\"Parsed output:\", response)\n</code></pre> <pre><code>Parsed output: LangChain is like a multilingual party where everyone speaks their own language but magically understands each other perfectly.\n</code></pre> <p>Para tener en cuenta</p> <p><code>StrOutputParser</code>  - Extrae el <code>.content</code> del objeto <code>AIMessage</code> generado por el LLM. - Nos garantiza que obtenemos solo el texto limpio sin informaci\u00f3n adicional.  </p> <p>\ud83d\udd39 Sin el parser: Obtenemos un objeto <code>AIMessage</code> y debemos extraer manualmente <code>.content</code>.  </p> <p>\ud83d\udd39 Con el parser: Recibimos directamente el texto limpio.  </p> <p>\u2705 Es un peque\u00f1o avance, pero marca la diferencia. Nos ahorra pasos manuales y sienta la base para mejoras m\u00e1s avanzadas.  </p>"},{"location":"Unidad%201/#commaseparatedlistoutputparser","title":"<code>CommaSeparatedListOutputParser</code>","text":"<p>Este parser toma una cadena de texto separada por comas y la convierte en una lista estructurada.</p> <p>Ejemplo: Entrada: <code>\"manzana, banana, cereza\"</code> Salida: <code>[\"manzana\", \"banana\", \"cereza\"]</code> </p> <p>Ve\u00e1moslo en detalle:</p> C\u00f3digoSalida <pre><code># Import the parser\nfrom langchain_core.output_parsers import CommaSeparatedListOutputParser\n\n# New prompt asking for a list\nlist_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that delivers comma-separated items.\"),\n    (\"human\", \"Give me 8 examples of {category}, separated by commas, omit additional comments only list the objects like a Python list.\")\n])\n\n# Create the chain with the new parser\nlist_parser = CommaSeparatedListOutputParser()\nlist_chain = list_prompt | llm | list_parser\n\n# Run it\nresponse = list_chain.invoke({\"category\": \"programming languages\"})\nprint(\"List output:\", response)\n</code></pre> <pre><code>List output: ['Python', 'Java', 'C++', 'JavaScript', 'Ruby', 'Swift', 'PHP', 'Go']\n</code></pre> <p>Puedes verificar que el tipo de la salida es una lista:</p> C\u00f3digoSalida <pre><code>print(type(response))\n</code></pre> <pre><code>&lt;class 'list'&gt;\n</code></pre> <p>Esto nos permite, por ejemplo, usar \u00edndices para acceder a los elementos de la lista:</p> C\u00f3digoSalida <pre><code>response[1]\n</code></pre> <pre><code>'Java'\n</code></pre> <p>Es decir, el LLM nos proporciona texto, pero el parser lo convierte en una lista que podemos usar en c\u00f3digo. Los parsers nos permiten agregar pasos adicionales a las cadenas de ejecuci\u00f3n; puedes pensarlo como una l\u00ednea de ensamblaje con el flujo:</p> <pre><code>[Prompt] --&gt; [LLM] --&gt; [Parser] --&gt; Structured Output\n</code></pre> Analog\u00eda de una cadena con parser. Las instrucciones se ejecutan en orden como en una l\u00ednea de ensamblaje. Fuente: Creado por Grok 3 (xAI) usando un prompt del usuario."},{"location":"Unidad%201/#jsonoutputparser","title":"<code>JsonOutputParser</code>","text":"<p>El <code>JsonOutputParser</code> es un parser que toma una cadena de texto en formato JSON y la convierte en un objeto de Python, como un diccionario o una lista, dependiendo de la estructura del JSON. Esto es particularmente \u00fatil cuando trabajamos con datos estructurados que vienen de una base de datos en la nube o de una API.</p> <p>Ejemplo:</p> C\u00f3digo <pre><code>from langchain_core.output_parsers import JsonOutputParser\n\n# Prompt asking for JSON with varied types\njson_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Return a JSON object with 'name' (string), 'age' (number or null), 'is_student' (true/false), and 'city' (string or null).\"),\n    (\"human\", \"Give me details for {person} in JSON format.\")\n])\n\n# Create the chain with the parser\njson_chain = json_prompt | llm | JsonOutputParser()\n\n# Chain without parser\nno_parse_chain = json_prompt | llm\n</code></pre> <p>Para ver m\u00e1s claramente lo que hace el <code>JsonOutputParser</code>, corramos la cadena sin el parser:</p> C\u00f3digo sin ParserSalida <pre><code>no_parse_response = no_parse_chain.invoke({\"person\": \"Alice\"}).content\nprint(\"Respuesta sin parser:\", no_parse_response)\nprint(\"Type:\", type(no_parse_response))\n</code></pre> <pre><code>Respuesta sin parser: {\"name\": \"Alice\", \"age\": 30, \"is_student\": false, \"city\": \"Paris\"}\nType: &lt;class 'str'&gt;\n</code></pre> <p>La respuesta es un <code>str</code>, es decir, texto simple, por lo que no puedo acceder a los elementos del JSON usando <code>keys</code>:</p> C\u00f3digoSalida <pre><code># Esto causar\u00e1 un error\nno_parse_response[\"name\"]\n</code></pre> <pre><code>TypeError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 no_parse_response[\"name\"]\n\nTypeError: string indices must be integers, not 'str'\n</code></pre> <p>Ahora, ejecutemos la cadena con el <code>JsonOutputParser</code>:</p> C\u00f3digo con ParserSalida <pre><code>json_response = json_chain.invoke({\"person\": \"Alice\"})\nprint(\"JsonOutputParser result:\", json_response)\nprint(\"Type:\", type(json_response))\n</code></pre> <pre><code>JsonOutputParser result: {'name': 'Alice', 'age': 30, 'is_student': False, 'city': 'Paris'}\nType: &lt;class 'dict'&gt;\n</code></pre> <p>Puedes acceder a los valores del JSON usando claves:</p> C\u00f3digoSalida <pre><code>json_response[\"city\"]\n</code></pre> <pre><code>'Paris'\n</code></pre> <p>El LLM produce JSON: <code>{\"name\": \"Alice\", \"age\": 30, \"is_student\": false, \"city\": \"Paris\"}</code>. <code>JsonOutputParser</code> convierte <code>null</code> en JSON a <code>None</code> en Python, <code>true/false</code> a <code>True/False</code>, y retorna un diccionario de Python.</p> <ul> <li> <p> Reto Formativo Planteamiento:       El siguiente es el comentario de un cliente en una tienda virtual:</p> <p><code>review_cliente = \"Compr\u00e9 los auriculares inal\u00e1mbricos XYZ y estoy muy satisfecho con mi compra. El tiempo de entrega fue excelente, ya que llegaron dos d\u00edas antes de lo previsto, lo cual super\u00f3 mis expectativas. En cuanto al precio, aunque hay opciones m\u00e1s econ\u00f3micas en el mercado, considero que la calidad del sonido, la duraci\u00f3n de la bater\u00eda y la comodidad justifican totalmente el coste. Adem\u00e1s, los compr\u00e9 como regalo para mi pareja y fueron un \u00e9xito total, gracias a su elegante presentaci\u00f3n y la impresionante calidad del sonido.\"</code></p> <p>Extraer informaci\u00f3n de la rese\u00f1a en un objeto JSON <code>Rese\u00f1as</code>.</p> <p>\ud83d\udce5 Entrada   Cadena <code>review_cliente</code> con la opini\u00f3n del cliente.</p> <p>\ud83d\udce4 Salida (JSON) <code>{     \"sentiment\": \"Positive\" | \"Negative\" | \"Neutral\",     \"delivery_time\": &lt;int&gt; | null,     \"quality_rating\": \"Good\" | \"Fair\" | \"Poor\" | null,     \"extra_comment\": &lt;string&gt; | null   }</code></p> </li> </ul>"},{"location":"Unidad%201/#structuredoutputparser-json-con-estructura-definida","title":"<code>StructuredOutputParser</code>: JSON con Estructura Definida","text":"<p>Este parser toma JSON y lo convierte en un diccionario de Python siguiendo un esquema espec\u00edfico que definimos con <code>ResponseSchema</code>. Solo extrae los campos que especificamos, garantizando una estructura clara y predecible. Veamos:</p> C\u00f3digoSalida <pre><code>from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n# Define the schema for the structured output\nschemas = [\n    ResponseSchema(name=\"sentiment\", description=\"Sentiment: Positive/Negative/Neutral\", type=\"string\"),\n    ResponseSchema(name=\"delivery_time\", description=\"Delivery time in days (or null if not mentioned)\", type=\"integer\"),\n    ResponseSchema(name=\"quality_rating\", description=\"Quality: Good/Fair/Poor (or null)\", type=\"string\"),\n    ResponseSchema(name=\"extra_comment\", description=\"Additional note (or null)\", type=\"string\")\n]\n\nstructured_parser = StructuredOutputParser.from_response_schemas(schemas)\n# Get the format instructions as a string\nformat_instructions = structured_parser.get_format_instructions()\n\n# Escape curly braces in format_instructions\nescaped_format_instructions = format_instructions.replace('{', '{{').replace('}', '}}')\n\n# Define the system message with escaped format instructions\nsystem_message = (\n    \"Analyze this review and return a JSON object. \"\n    \"Format: ```json\\n\"\n    f\"{escaped_format_instructions}\\n\"\n    \"```\"\n)\n\n# Create the prompt template with {review} as the only variable\nstructured_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_message),\n    (\"human\", \"Review: {review}\")\n])\n\n# Create the chain \nstructured_chain = structured_prompt | llm | structured_parser\n\n# Run the chain with the review (assuming review_cliente is defined)\nstructured_response = structured_chain.invoke({\"review\": review_cliente})\nprint(\"StructuredOutputParser result:\", structured_response)\nprint(\"Type:\", type(structured_response))\n</code></pre> <pre><code>StructuredOutputParser result: {'sentiment': 'Positive', 'delivery_time': 2, 'quality_rating': 'Good', 'extra_comment': 'The headphones were a total success as a gift for my partner, thanks to their elegant presentation and impressive sound quality.'}\nType: &lt;class 'dict'&gt;\n</code></pre> <p>En este c\u00f3digo, el usuario final del mensaje estructurado es el modelo de lenguaje (LLM). Este mensaje est\u00e1 estructurado de tal manera que incluye instrucciones de ensamblaje para que el LLM procese el formato correctamente. La funci\u00f3n que nos permite especificar estas instrucciones es <code>get_format_instructions()</code>.</p> <p>La funci\u00f3n <code>get_format_instructions()</code> crea un string que contiene las instrucciones de formato basadas en los objetos <code>ResponseSchema</code>. Este string describe c\u00f3mo debe estructurarse la salida del modelo de lenguaje (LLM) para que sea f\u00e1cil de interpretar y procesar posteriormente.</p> <p>En analog\u00eda con el constructor de la clase <code>ChatPromptTemplate.from_template</code>, que describimos como un carpintero que crea un caj\u00f3n a partir de maderas brutas, este tipo de template con instrucciones de formato se asemejar\u00eda a construir un caj\u00f3n modular con instrucciones de armado, como el de la figura: </p> <p> Analog\u00eda de un template con instrucciones de formato. Fuente:  Captura de pantalla de internet. </p> <p>Aqu\u00ed termina nuestra primera unidad. \u00a1Felicidades por llegar hasta el final! Ahora conoces el contexto general de las tecnolog\u00edas involucradas en el desarrollo de aplicaciones asistidas por IA. Este campo apenas est\u00e1 comenzando, y ahora tienes las bases para utilizar herramientas m\u00e1s sofisticadas, como las cadenas y la gesti\u00f3n de memoria, que ser\u00e1n el tema de la siguiente unidad.</p>"},{"location":"Unidad%201/#glosario","title":"Glosario","text":"<ol> <li> <p>API (Application Programming Interface):    Un conjunto de definiciones y protocolos que permite a las aplicaciones comunicarse entre s\u00ed. Las APIs facilitan la integraci\u00f3n y el intercambio de datos entre diferentes sistemas y servicios.</p> </li> <li> <p>Parser:    Un componente que analiza y transforma texto en un formato estructurado, facilitando su manipulaci\u00f3n y an\u00e1lisis en aplicaciones de software.</p> </li> <li> <p>JSON (JavaScript Object Notation):    Un formato de intercambio de datos ligero y f\u00e1cil de leer que utiliza una estructura basada en pares clave-valor. Com\u00fanmente usado para representar objetos en aplicaciones web.</p> </li> <li> <p>LangChain:    Un marco de trabajo que permite la creaci\u00f3n de aplicaciones asistidas por IA mediante la integraci\u00f3n de modelos de lenguaje con herramientas y flujos de trabajo personalizados.</p> </li> <li> <p>ChatPromptTemplate:    Una plantilla utilizada para crear mensajes estructurados que se env\u00edan a los modelos de lenguaje, permitiendo la personalizaci\u00f3n de las interacciones.</p> </li> <li> <p>Output Parser:    Un tipo de parser que se utiliza para definir c\u00f3mo debe estructurarse la salida de un modelo de lenguaje, asegurando que sea f\u00e1cil de interpretar y procesar.</p> </li> <li> <p>Ventana de contexto:    La cantidad de texto que un modelo de lenguaje puede procesar a la vez. Limita la cantidad de informaci\u00f3n que puede ser considerada en una sola interacci\u00f3n.</p> </li> <li> <p>LangChain Expression Language (LCEL):    Un lenguaje de expresi\u00f3n utilizado en LangChain para definir y manipular flujos de trabajo y cadenas de procesamiento de manera eficiente.</p> </li> <li> <p>Pipeline (Cadena de Procesamiento):    Un flujo de trabajo secuencial donde los datos pasan por diferentes etapas o componentes, cada uno realizando una tarea espec\u00edfica.</p> </li> <li> <p>Memoria en cadenas de conversaci\u00f3n:     La capacidad de un sistema de inteligencia artificial para almacenar y utilizar informaci\u00f3n de interacciones pasadas, mejorando la coherencia y personalizaci\u00f3n en futuras interacciones.</p> </li> </ol>"},{"location":"Unidad%201/#evidencia-de-aprendizaje","title":"Evidencia de Aprendizaje","text":"Unidad 1 Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLMs EA1. Templates y Output Parsers <p>Ejercicio 1 - Correcci\u00f3n de respuestas inapropiadas en atenci\u00f3n al cliente:</p> <p>En este ejercicio, debes usar la IA para mejorar respuestas inapropiadas escritas por un operador de servicio al cliente. La IA corregir\u00e1 el tono, la cortes\u00eda y errores ortogr\u00e1ficos, asegurando que la respuesta sea adecuada para el cliente. Input: Un mensaje del cliente y una respuesta inapropiada del operador. Output: Una respuesta final corregida y apropiada para enviar al cliente. Requisitos: - Utiliza un prompt template para generar la respuesta apropiada. - Implementa un output parser para validar que la respuesta cumple con los criterios de cortes\u00eda y ortograf\u00eda. Bonus: Investiga sobre memoria.  Si el cliente ha escrito varios mensajes, utiliza memoria para recordar el contexto de la conversaci\u00f3n.</p> <p>Ejercicio 2 (F\u00e1cil) - Extracci\u00f3n de informaci\u00f3n clave en rese\u00f1as de productos:</p> <p>Dado un review de un producto en un sitio de e-commerce, crea un modelo que extraiga informaci\u00f3n espec\u00edfica. Tareas: - Identificar si el producto fue comprado como regalo. - Extraer la opini\u00f3n del cliente sobre el precio. - Extraer comentarios sobre el tiempo de entrega. Requisitos: - Utiliza output parsers para extraer los campos relevantes en forma de estructuras de datos de Python como un diccionario. - Dise\u00f1a un prompt template que permita a la IA identificar y organizar estos elementos de manera eficiente.</p> <p>Guarda los documentos con la siguiente nomenclatura:</p> <ul> <li>Apellido_Nombre del estudiante.ipynb Ejemplo: </li> <li>L\u00f3pez_Karla.ipynb</li> </ul> <p>Finalmente, haz clic en el bot\u00f3n Cargar Tarea, sube tu archivo y presiona el bot\u00f3n Enviar para remitirlo a tu profesor con el fin de que lo eval\u00fae y retroalimente. |</p> <p>\ud83d\udcd6 Nota</p> <p>Conoce los criterios de evaluaci\u00f3n de esta evidencia de aprendizaje consultando la r\u00fabrica que encontrar\u00e1s a continuaci\u00f3n.</p> Criterios Ponderaci\u00f3n Totales 70 50 5 0 Calidad de las Soluciones Las soluciones a los ejercicios son correctas, demostrando una implementaci\u00f3n adecuada de los conceptos y t\u00e9cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci\u00f3n adecuada de los conceptos y t\u00e9cnicas involucradas. Hay evidencia de esfuerzo y comprensi\u00f3n de los temas. Las soluciones presentadas son en su mayor\u00eda incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi\u00f3n de los conceptos y t\u00e9cnicas esenciales. No realiza la entrega 70 Calidad de la entrega El notebook es claro y f\u00e1cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c\u00f3digo en las celdas Markdown, lo que facilita la comprensi\u00f3n de las soluciones propuestas. El notebook no es particularmente f\u00e1cil de leer, pero a\u00fan as\u00ed incluye comentarios que explican el funcionamiento del c\u00f3digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l\u00f3gica detr\u00e1s del c\u00f3digo. El notebook carece de comentarios acerca del funcionamiento del c\u00f3digo en las celdas Markdown, lo que dificulta la comprensi\u00f3n de las soluciones implementadas. No realiza la entrega 20 Tiempo de la entrega La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci\u00f3n de la actividad. La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. La entrega se realiza con m\u00e1s de una semana de atraso, lo que indica un retraso significativo en la presentaci\u00f3n de la actividad. No realiza la entrega 10 Ponderaci\u00f3n de la actividad 100 puntos"},{"location":"Unidad%201/#referencias","title":"Referencias","text":"<p>Chase, H., &amp; Ng, A. (2023). LangChain for LLM Application Development [Curso en l\u00ednea]. DeepLearning.AI. Disponible en https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</p> <p>Huyen, C. (2025). AI Engineering: Building Applications with Foundation Models (1.\u00aa ed.). O'Reilly Media. Disponible en https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302</p> <p>LangChain. (s.f.). LangChain Documentation. Disponible en https://python.langchain.com/docs/introduction/</p> <p>Te invitamos a explorar el siguiente material para ampliar tus conocimientos sobre modelos de lenguaje (LLMs), LangChain, plantillas de prompts y parsers de salida. Estos recursos te proporcionar\u00e1n una comprensi\u00f3n m\u00e1s profunda y pr\u00e1ctica de los temas abordados en el curso.</p>"},{"location":"Unidad%201/#lecturas-y-material-complementario","title":"Lecturas y material complementario","text":""},{"location":"Unidad%201/#lecturas-recomendadas","title":"\ud83d\udcda Lecturas recomendadas","text":""},{"location":"Unidad%201/#titulo-langchain-for-llm-application-development","title":"T\u00edtulo: LangChain for LLM Application Development","text":"<p>Autor: Harrison Chase &amp; Andrew Ng URL: LangChain for LLM Application Development Este curso gratuito de DeepLearning.AI ofrece una introducci\u00f3n pr\u00e1ctica al desarrollo de aplicaciones con modelos de lenguaje utilizando LangChain. Cubre temas como plantillas de prompts, parsers de salida y encadenamiento de componentes.</p>"},{"location":"Unidad%201/#titulo-langchain-output-parser-guide","title":"T\u00edtulo: LangChain Output Parser Guide","text":"<p>Autor: Restack URL: LangChain Output Parser Guide Este art\u00edculo profundiza en el uso de los parsers de salida de LangChain, explicando c\u00f3mo transformar las respuestas de los modelos de lenguaje en formatos estructurados como JSON, y c\u00f3mo integrarlos en aplicaciones pr\u00e1cticas.</p>"},{"location":"Unidad%201/#titulo-prompt-template-langchain-opentutorial","title":"T\u00edtulo: Prompt Template | LangChain OpenTutorial","text":"<p>Autor: Hye-yoon Jeong URL: Prompt Template | LangChain OpenTutorial Este tutorial cubre c\u00f3mo crear y utilizar plantillas de prompts en LangChain, esenciales para generar prompts din\u00e1micos y flexibles que se adapten a diversos casos de uso.</p>"},{"location":"Unidad%201/#titulo-jsonoutputparser-langchain-opentutorial","title":"T\u00edtulo: JsonOutputParser | LangChain OpenTutorial","text":"<p>Autor: LangChain OpenTutorial URL: JsonOutputParser | LangChain OpenTutorial Este tutorial muestra c\u00f3mo utilizar el <code>JsonOutputParser</code> de LangChain para estructurar las salidas de los modelos de lenguaje en formato JSON, facilitando su integraci\u00f3n en aplicaciones que requieren datos estructurados.</p>"},{"location":"Unidad%201/#videos-recomendados","title":"\ud83c\udfa5 Videos recomendados","text":""},{"location":"Unidad%201/#titulo-transformers-how-llms-work-explained-visually-dl5","title":"T\u00edtulo: Transformers (how LLMs work) explained visually | DL5","text":"<p>Autor: 3Blue1Brown URL: Transformers (how LLMs work) explained visually Este video ofrece una explicaci\u00f3n visual de c\u00f3mo funcionan los modelos de lenguaje grandes (LLMs) mediante la arquitectura de transformers, facilitando la comprensi\u00f3n de conceptos complejos.</p>"},{"location":"Unidad%201/#titulo-attention-in-transformers-step-by-step-dl6","title":"T\u00edtulo: Attention in transformers, step-by-step | DL6","text":"<p>Autor: 3Blue1Brown URL: Attention in transformers, step-by-step Este video desglosa paso a paso el mecanismo de atenci\u00f3n en los transformers, una parte crucial en el funcionamiento de los LLMs.</p>"},{"location":"Unidad%201/#titulo-how-might-llms-store-facts-dl7","title":"T\u00edtulo: How might LLMs store facts | DL7","text":"<p>Autor: 3Blue1Brown URL: How might LLMs store facts Este video explora c\u00f3mo los modelos de lenguaje grandes pueden almacenar hechos y conocimientos, proporcionando una visi\u00f3n m\u00e1s profunda de su funcionamiento interno.</p>"},{"location":"Unidad2/unidad2/","title":"Unidad 2","text":""},{"location":"Unidad2/unidad2/#unidad-2-cadenas-y-memoria","title":"Unidad 2: Cadenas y Memoria","text":""},{"location":"Unidad2/unidad2/#introduccion-a-la-unidad","title":"Introducci\u00f3n a la unidad","text":"<p>Bienvenidos a la segunda unidad de nuestro diplomado sobre construcci\u00f3n de aplicaciones asistidas por LLMs. En la primera unidad aprendiste a confeccionar instrucciones reutilizables para LLMs, los prompt templates, y exploraste c\u00f3mo acoplar estas instrucciones en cadenas con especificadores de formato llamados output parsers. Sin embargo, estas cadenas de ejecuci\u00f3n eran cadenas de un solo turno de interacci\u00f3n entre la IA y el usuario humano. En esta unidad aprender\u00e1s a darle memoria y contexto a tus cadenas de ejecuci\u00f3n. Profundizaremos a\u00fan m\u00e1s en el funcionamiento de las cadenas y como limitar su memoria en el contexto del LCEL. Finalmente, pondr\u00e1s a prueba tu conocimiento creando un chatbot que asiste las labores de un m\u00e9dico realizando tareas secuenciales y que tiene como contexto en su memoria los datos espec\u00edficos de un paciente.</p> <p>\u00a1Comencemos!</p>"},{"location":"Unidad2/unidad2/#resultados-de-aprendizaje","title":"Resultados de aprendizaje","text":"<p>Al finalizar esta unidad, estar\u00e1s en capacidad de:</p> <ul> <li>Usar cadenas dotadas de memoria usando el LECL.</li> <li>Limitar el tama\u00f1o del contexto cargado en la memoria de tus cadenas.</li> </ul>"},{"location":"Unidad2/unidad2/#cronograma-de-actividades-unidad-1","title":"Cronograma de actividades - Unidad 1","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n EA1:  Cadenas y memoria EA1: Cadenas y memoria Semana 4 y 5 25% Total 25 %"},{"location":"Unidad2/unidad2/#cadenas","title":"Cadenas","text":"<p>Hay varias maneras de instanciar cadenas de ejecuci\u00f3n en LangChain, algunas de las cuales fueron exploradas en la unidad 1. Recientemente, LangChain introdujo LangChain Expression Language (LCEL) como el est\u00e1ndar para construir cadenas. Revisemos m\u00e1s detalladamente de qu\u00e9 se trata:</p> <p>Para tener en cuenta</p> <p>Recuerda cargar tu llave en las variables de sistema si no lo has hecho: <pre><code>import os\nimport openai\nfrom dotenv import load_dotenv, find_dotenv\n\n# Cargar el archivo .env local\n_ = load_dotenv(find_dotenv()) \nopenai.api_key = os.environ['OPENAI_API_KEY']\n</code></pre></p>"},{"location":"Unidad2/unidad2/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":"<p>LangChain Expression Language (LCEL) es una sintaxis para definir cadenas. Permite componer objetos ejecutables (runnables)\u2014objetos que pueden ser ejecutados o encadenados\u2014usando el operador pipe (|). Un runnable es cualquier componente que implementa la interfaz Runnable, lo que significa que puede procesar entradas y producir salidas. Ejemplos incluyen:</p> <ul> <li>Plantillas de prompt (ChatPromptTemplate), como lo hicimos en la unidad 1.</li> </ul> <p>El operador de tuber\u00eda (|) conecta estos componentes, pasando la salida de un Runnable como la entrada al siguiente. Por ejemplo:</p> <pre><code>chain = prompt | llm | output_parser\n</code></pre> <p>Veamos un ejemplo en detalle:</p> C\u00f3digoSalida <pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Define the prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a world-class technical documentation writer.\"),\n    (\"user\", \"{input}\")\n])\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n\n# Create the output parser\noutput_parser = StrOutputParser()\n\n# Compose the chain\nchain = prompt | llm | output_parser\n\n# Invoke the chain\nresponse = chain.invoke({\"input\": \"How can LangSmith help with testing?\"})\nprint(response)\n</code></pre> <pre><code>LangSmith provides a suite of tools designed to assist with software testing, particularly in the realm of language translation and localization. It can be invaluable for teams developing software that needs to function in multiple languages and regions.\n\n1. **Automated Testing:** LangSmith can automatically test your software\u2019s language functionality, ensuring translations are accurate, context-appropriate, and that all elements of the user interface are correctly localized.\n\n2. **Quality Assurance:** By checking translations against a comprehensive database, LangSmith aids in maintaining high translation quality, reducing the risk of miscommunication or confusion for users.\n\n3. **Regression Testing:** When updates or changes are made to the software, LangSmith can help ensure that these changes have not negatively affected the language functionality.\n\n4. **Cultural Accuracy:** Beyond simple language translation, LangSmith can also verify that cultural nuances and local customs are appropriately considered, which can greatly improve user experience.\n\n5. **Reporting and Analytics:** LangSmith provides detailed reports on testing outcomes, highlighting any issues or potential areas for improvement. This can provide valuable insights for your development team.\n\nBy integrating LangSmith into your testing process, you can more effectively ensure the quality and accuracy of your software's multilingual and multicultural features, improving overall user experience and satisfaction.\n</code></pre>"},{"location":"Unidad2/unidad2/#funciones-envueltas-con-runnablelambda","title":"Funciones (envueltas con RunnableLambda)","text":"<p>Podemos usar cadenas con funciones al comienzo de la l\u00ednea de ejecuci\u00f3n, un cierto tipo de funci\u00f3n muy vers\u00e1til son las funciones de la clase RunnableLambda. Estas est\u00e1n dise\u00f1adas para integrar funciones personalizadas de Python en cadenas de LangChain. Permiten a los desarrolladores envolver funciones de Python arbitrarias (o funciones lambda) en un objeto ejecutable, haci\u00e9ndolas compatibles con la sintaxis del LCEL. Es decir, convierte una funci\u00f3n de Python en un componente encadenable.</p> <p>La funci\u00f3n debe aceptar una entrada compatible con la salida del paso anterior y producir una salida compatible con el siguiente paso.</p> <p>Por ejemplo:</p> <p><pre><code>from langchain_core.runnables import RunnableLambda\n\n# Wrap a function\nrunnable_sumaUno = RunnableLambda(lambda x: x + 1)  # suma 1 a la entrada\nrunnable_cuadrado = RunnableLambda(lambda x: x**2)  # eleva al cuadrado la entrada\n\n# La cadena de ejecuci\u00f3n ser\u00eda\nchain = runnable_sumaUno | runnable_cuadrado  # encadena las funciones\nresult = chain.invoke(5)  # (5 + 1) ** 2 = 36\nprint(result)  # Output: 36\n</code></pre> Vemos otro ejemplo:</p> <pre><code>from langchain_core.runnables import RunnableLambda\n\n# Define funciones lambda individuales\nadd_prefix = RunnableLambda(lambda x: f\"Hello, {x}!\")\nto_upper = RunnableLambda(lambda x: x.upper())\n\n# Construir la cadena\nchain = add_prefix | to_upper\n\n# Invocar la cadena\nresult = chain.invoke(\"Alice\")\nprint(result)  # Salida: HELLO, ALICE!\n</code></pre> <p>Veamos c\u00f3mo construir una cadena que toma una consulta de usuario, la formatea en un prompt, la procesa con un LLM y extrae la primera palabra de la respuesta.</p> <pre><code>from langchain_core.runnables import RunnableLambda\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI  # o usar otro LLM\n\n# Definir componentes\n\n# Crear un prompt template y llenarlo\nformat_prompt = RunnableLambda(\n    lambda x: PromptTemplate.from_template(\"Answer briefly: {query}\").format(query=x)\n)\n\n# Extraer la primera palabra de la respuesta\nextract_first_word = RunnableLambda(\n    lambda x: x.content.split()[0] if hasattr(x, 'content') else x.split()[0]\n)\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n# Construir la cadena\nchain = format_prompt | llm | extract_first_word\n\n# Invocar la cadena\nresult = chain.invoke(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\nprint(result)  # Salida: Iv\u00e1n (recuerda que gpt-3.5 fue entrenado en datos hasta octubre de 2023)\n</code></pre> <p>La cadena procesa la consulta de entrada paso a paso: formatea la consulta, la procesa con el LLM y extrae la primera palabra de la respuesta. Puedes reemplazar <code>ChatOpenAI</code> con otro modelo (por ejemplo, <code>HuggingFaceHub</code>) si es necesario. Language models (ChatOpenAI).</p>"},{"location":"Unidad2/unidad2/#memoria","title":"Memoria","text":"<p>La memoria es el mecanismo por el cual le damos al LLM contexto de nuestras interacciones previas.</p> <p>Para explorar el uso de la memoria, instanciaremos una cadena de conversaci\u00f3n a partir de la clase preconstruida <code>ConversationChain</code>. Importamos los m\u00f3dulos necesarios:</p> <pre><code>from langchain.chains.conversation.base import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_openai import ChatOpenAI\n</code></pre> <p>Usaremos GPT-4 y el modelo de chat de OpenAI:</p> <pre><code>chat_model = ChatOpenAI(model_name=\"gpt-4\", temperature=0, streaming=True)\n</code></pre> <p>Para instanciar una cadena de conversaci\u00f3n de la clase <code>ConversationChain</code>, necesitamos instanciar primero el objeto de clase <code>Memory</code> que nos servir\u00e1 para administrar la memoria de las interacciones en la conversaci\u00f3n. Lo hacemos de la siguiente manera:</p> <pre><code>memory = ConversationBufferMemory() # Es una instancia de la clase que contiene los controles de la memoria\n</code></pre> <p>As\u00ed, la cadena de conversaci\u00f3n la instanciaremos como:</p> <p><pre><code>chain = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n</code></pre> Ahora est\u00e1 todo listo para que comencemos nuestra conversaci\u00f3n:</p> C\u00f3digoSalida <pre><code>chain.invoke(\"Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\")\n</code></pre> <pre><code>&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI:\n\n&gt; Finished chain.\n\n    {'input': 'Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?',\n'history': '',\n'response': '\u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?'}\n</code></pre> <p>Si pregunto aluna cosa adicional, por ejeplo cuando es 2 + 2. No olvidar\u00e1 mi nombre:</p> C\u00f3digoSalida <pre><code>chain.invoke(\"cuanto es 2 + 2?\")\nchain.invoke(\"cual es mi nombre?\")\n</code></pre> <pre><code>&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?\nHuman: cuanto es 2 + 2?\nAI: 2 + 2 es igual a 4. Es una de las operaciones matem\u00e1ticas m\u00e1s b\u00e1sicas y es un buen ejemplo de c\u00f3mo funcionan las sumas. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otro tema, estar\u00e9 encantado de ayudarte.\nHuman: \u00bfCu\u00e1l es mi nombre?\nAI:\n\n{'input': '\u00bfCu\u00e1l es mi nombre?',\n'history': 'Human: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?\\nHuman: cuanto es 2 + 2?\\nAI: 2 + 2 es igual a 4. Es una de las operaciones matem\u00e1ticas m\u00e1s b\u00e1sicas y es un buen ejemplo de c\u00f3mo funcionan las sumas. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otro tema, estar\u00e9 encantado de ayudarte.',\n'response': 'Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci\u00f3n. Si tienes m\u00e1s preguntas o necesitas ayuda con algo m\u00e1s, no dudes en dec\u00edrmelo.'}\n</code></pre> <p>La IA responde que Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci\u00f3n. Lo cual no era posible en la unidad 1 cuando invoc\u00e1bamos el modelo sin memoria.</p>"},{"location":"Unidad2/unidad2/#la-ventana-de-contexto","title":"La ventana de contexto","text":"<p>La memoria es un recurso costoso, pues los modelos tienen una capacidad limitada para guardar el contexto de las conversaciones. Esta caracter\u00edstica es llamada la ventana de contexto. A medida que los modelos se han vuelto m\u00e1s avanzados, las ventanas de contexto ofrecidas son cada vez m\u00e1s grandes. </p> <ul> <li>GPT-3: 4096 tokens</li> <li>GPT-3.5-turbo: 4096 tokens</li> <li>GPT-4: 8192 tokens (con una versi\u00f3n extendida de 32768 tokens)</li> </ul> <p>Sin embargo, debemos tener en cuenta que el modelo siempre tratar\u00e1 de relacionar sus respuestas con el contexto de la conversaci\u00f3n. Esto puede ser beneficioso, pero al mismo tiempo, puede introducir ruido a la conversaci\u00f3n, y el modelo puede generar respuestas menos precisas si est\u00e1 distra\u00eddo en un contexto muy amplio. Por este motivo, resulta conveniente tener la posibilidad de limitar la memoria que queremos dar a las aplicaciones asistidas por IA. Veremos ahora algunos tipos de memoria y sus usos.</p>"},{"location":"Unidad2/unidad2/#tipos-de-memoria-en-langchain","title":"Tipos de Memoria en LangChain","text":"<p>LangChain ofrece varios tipos de memoria que se pueden utilizar para gestionar el contexto en las aplicaciones de inteligencia artificial</p> <p>crearemos una pequena encapsulaci\u00f3n para ilistrar el uso de las dos clases principales:</p> <pre><code>class ChatBot:\n    def __init__(self, memory):\n        self.chat_model  =  ChatOpenAI(model_name= \"gpt-4o\", temperature= 0, streaming= True)\n        self.memory = memory\n        self.chain = ConversationChain(llm=self.chat_model, memory=self.memory, verbose=True)\n\n# metodo para responder\n    def pregunta(self, pregunta: str):        \n        response = self.chain.predict(input = pregunta)\n        print(response)\n        return 0\n</code></pre>"},{"location":"Unidad2/unidad2/#tipos-de-memoria-en-langchain_1","title":"Tipos de Memoria en LangChain","text":"<p>ConversationBufferMemory:     - Uso: Esta memoria almacena todo el historial de la conversaci\u00f3n sin ning\u00fan l\u00edmite. Es \u00fatil cuando se desea mantener un registro completo de todas las interacciones previas, como lo hicimos en el ejemplo anterior.</p> <p>Por ejemplo, un chat con memoria ilimitada se crear\u00eda de la siguiente manera:</p> <p><pre><code>from langchain.memory import ConversationBufferMemory\n\nmucha_memoria = ConversationBufferMemory()\ngenioBot = ChatBot(mucha_memoria)\n</code></pre> ConversationBufferWindowMemory:    - Uso: Similar a <code>ConversationBufferMemory</code>, pero con un par\u00e1metro llamado <code>window_size</code> que permite recordar solo un n\u00famero fijo de interacciones recientes.</p> <p>Creamos un bot muy inteligente pero con memoria limitada usando <code>ConversationBufferWindowMemory</code>:</p> C\u00f3digoSalida <pre><code>from langchain.memory import ConversationBufferWindowMemory\n\n# Memoria con ventana de 1\npoca_memoria = ConversationBufferWindowMemory(k=1)\n\n# Instanciamos un bot con poca memoria\nolvidoBot = ChatBot(poca_memoria)\nolvidoBot.pregunta(\"Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\")\nolvidoBot.pregunta(\"\u00bfCu\u00e1nto es 2 + 5?\")\nolvidoBot.pregunta(\"\u00bfCu\u00e1l es mi nombre?\") # No sabe el nombre\n</code></pre> <pre><code>&gt; Entering new ChatBot session...\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial dise\u00f1ada para ayudarte con informaci\u00f3n y responder a tus preguntas. \u00bfEn qu\u00e9 puedo asistirte hoy?\n\nHuman: \u00bfCu\u00e1nto es 2 + 5?\nAI: 2 + 5 es igual a 7. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otra cosa en mente, \u00a1estar\u00e9 encantado de ayudarte!\n\nHuman: \u00bfCu\u00e1l es mi nombre?\nAI: Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo hayas dicho antes en esta conversaci\u00f3n. Si quieres, puedes dec\u00edrmelo ahora y lo recordar\u00e9 para el resto de nuestra charla.\n</code></pre> <p><code>olvidoBot</code> solo recordar\u00e1 una interacci\u00f3n a la vez debido a <code>window_size=1</code>. Cuando preguntamos \"\u00bfCu\u00e1l es mi nombre?\", ya no recuerda la interacci\u00f3n donde le dijimos nuestro nombre, por lo que no puede recordarlo.</p> <p><code>ConversationSummaryMemory</code> almacena un resumen de la conversaci\u00f3n, ideal para interacciones muy largas donde no es necesario retener todos los detalles. </p> <p>Para tener en cuenta</p> <p>Las gu\u00edas m\u00e1s recientes recomiendan utilizar la funci\u00f3n <code>trim_messages</code>, que proporciona una forma flexible de gestionar el historial de la conversaci\u00f3n. Las funcionalidades aqu\u00ed expuestas est\u00e1n siendo migradas a la plataforma de LangGraph, por lo que est\u00e1n fuera del alcance de este curso. Ver\u00e1s el mensaje <code>LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/</code>.</p>"},{"location":"Unidad2/unidad2/#entendiendo-la-funcion-trim_messages","title":"Entendiendo la Funci\u00f3n <code>trim_messages</code>","text":"<p>La funci\u00f3n <code>trim_messages</code> es una utilidad en LangChain dise\u00f1ada para reducir el tama\u00f1o de un historial de chat a un n\u00famero espec\u00edfico de tokens o mensajes, asegurando que el historial recortado siga siendo v\u00e1lido para los modelos de chat. Un historial de chat v\u00e1lido t\u00edpicamente:</p> <ul> <li>Comienza con:</li> <li>Un <code>HumanMessage</code>, o</li> <li> <p>Un <code>SystemMessage</code> seguido de un <code>HumanMessage</code>.</p> </li> <li> <p>Termina con:</p> </li> <li>Un <code>HumanMessage</code>, o</li> <li>Un <code>ToolMessage</code> (com\u00fan en conversaciones basadas en agentes).</li> </ul> <p>Al recortar mensajes m\u00e1s antiguos o menos relevantes, <code>trim_messages</code> ayuda a enfocar el modelo en el contexto reciente y pertinente, evitanto informaci\u00f3n que que pueda distraer al modelo (no simepre un contexto grande es mejor).</p> <p>La funci\u00f3n <code>trim_messages</code> opera tomando varios par\u00e1metros para controlar c\u00f3mo se realiza el recorte:</p> Par\u00e1metro Descripci\u00f3n <code>messages</code> La secuencia de mensajes (por ejemplo, <code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>) a recortar. <code>max_tokens</code> El n\u00famero m\u00e1ximo de tokens que deben tener los mensajes recortados. <code>strategy</code> La estrategia de recorte: \"first\" (mantiene los primeros mensajes) o \"last\" (mantiene los m\u00e1s recientes, a menudo preferido para conversaciones). <code>token_counter</code> Una funci\u00f3n o LLM utilizado para contar tokens en los mensajes, como el m\u00e9todo de conteo de tokens incorporado de un LLM. <code>include_system</code> Un booleano (por defecto: <code>False</code>) que especifica si se debe mantener el <code>SystemMessage</code> al principio si est\u00e1 presente. <code>allow_partial</code> Un booleano (por defecto: <code>False</code>) que permite dividir un mensaje si solo parte de \u00e9l puede incluirse para cumplir con el l\u00edmite de tokens. <p>La funci\u00f3n devuelve una lista de mensajes recortados que se ajustan a los l\u00edmites especificados mientras mantienen la coherencia del contexto.</p> <p>Consideremos un ejemplo pr\u00e1ctico donde tenemos un chatbot que utiliza una cadena para procesar consultas de usuario. El chatbot ha estado funcionando durante varias interacciones, y el historial de chat se ha alargado. Necesitamos recortar el historial para ajustarlo a un l\u00edmite de 100 tokens, manteniendo los mensajes m\u00e1s recientes y el <code>SystemMessage</code>.</p> <p>Primero, definamos un historial de chat de ejemplo:</p> <pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nchat_history = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hello, my name is Juan how are you?\"),\n    AIMessage(content=\"I'm doing well, thank you. How can I help you today?\"),\n    HumanMessage(content=\"Can you tell me about the weather today?\"),\n    AIMessage(content=\"Sure, let me check that for you. [checks weather] It's sunny with a high of 75 degrees.\"),\n    HumanMessage(content=\"What's the capital of France?\"),\n    AIMessage(content=\"The capital of France is Paris.\"),\n]\n</code></pre> <p>Ahora, usaremos <code>trim_messages</code> para recortar este historial a 100 tokens, manteniendo los mensajes m\u00e1s recientes e incluyendo el <code>SystemMessage</code>:</p> <pre><code>from langchain_core.messages.utils import trim_messages\nfrom langchain_openai import OpenAI\n\n# Inicializar un LLM para el conteo de tokens\nllm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n\n# Recortar los mensajes\ntrimmed_history = trim_messages(\n    messages=chat_history,\n    max_tokens=100,\n    strategy=\"last\",\n    token_counter=llm.get_num_tokens_from_messages,\n    include_system=True, # para mantener el SystemMessage en la memoria\n)\n</code></pre> <p>Si el conteo total de tokens del historial original excede los 100, el historial recortado podr\u00eda verse as\u00ed:</p> <pre><code>[\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What's the capital of France?\"),\n    AIMessage(content=\"The capital of France is Paris.\"),\n]\n</code></pre> <p>Esto mantiene la conversaci\u00f3n enfocada y dentro de la ventana de contexto especificada, pero no recordar\u00e1 nuestro nombre. A continuaci\u00f3n, se presenta el c\u00f3digo completo:</p> C\u00f3digoSalida <pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.messages.utils import trim_messages\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Inicializar el LLM\n# Reemplazar 'your-openai-api-key' con tu clave API real de OpenAI\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n\n# Definir la plantilla de prompt\n# La plantilla incluye un mensaje de sistema, un marcador de posici\u00f3n para el historial de chat y la entrada del usuario\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"placeholder\", \"{chat_history}\"),  # Importante: este es el marcador de posici\u00f3n para el historial de chat\n    (\"human\", \"{input}\")\n])\n\n# Definir una funci\u00f3n para recortar mensajes\n# Esta funci\u00f3n recorta el historial de chat para ajustarse a un l\u00edmite de tokens especificado\ndef trim_chat_history(messages):\n    return trim_messages(\n        messages=messages,\n        max_tokens=100,  # Limitar a 100 tokens para ajustarse al contexto del modelo\n        strategy=\"last\",  # Conservar los mensajes m\u00e1s recientes\n        token_counter=llm.get_num_tokens_from_messages,  # Usar el contador de tokens del LLM\n        include_system=True  # Preservar el mensaje del sistema\n    )\n</code></pre> <pre><code>LLM Response: You haven't provided your name. Can you please share it with me?\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>El <code>(\"placeholder\", \"{chat_history}\")</code> en el <code>ChatPromptTemplate</code> es un componente clave del sistema de plantillas de prompt de LangChain, utilizado para definir un espacio en el prompt donde se insertar\u00e1 una secuencia de mensajes (por ejemplo, el historial de la conversaci\u00f3n).</p> Detalles <p>message type and its content or template.</p> <p>Tipos comunes de mensajes incluyen: - <code>(\"system\", \"...\")</code>: Un mensaje del sistema que define el rol o las instrucciones del asistente. - <code>(\"human\", \"...\")</code>: Un mensaje de entrada del usuario. - <code>(\"ai\", \"...\")</code>: Un mensaje de respuesta del asistente. - <code>(\"placeholder\", \"{variable_name}\")</code>: Un marcador de posici\u00f3n para una secuencia de mensajes o datos que se proporcionar\u00e1n m\u00e1s tarde.</p> <p>La tupla <code>(\"placeholder\", \"{chat_history}\")</code> indica que la variable llamada <code>chat_history</code> contendr\u00e1 una lista de mensajes (por ejemplo, <code>SystemMessage</code>, <code>HumanMessage</code>, <code>AIMessage</code>) que se insertar\u00e1n en esa posici\u00f3n en el prompt.</p> <p>Esta cadena procesa una lista \u00fanica de mensajes como contexto, pero no es propiamente una memoria de la conversaci\u00f3n, ya que es fija. Idealmente, queremos que la memoria sea persistente, es decir, que recuerde un cierto n\u00famero de interacciones. Para este prop\u00f3sito, podemos integrar <code>RunnableWithMessageHistory</code> a nuestra cadena para almacenar el historial de la conversaci\u00f3n.</p>"},{"location":"Unidad2/unidad2/#anadiendo-memoria-persistente-para-chatbots","title":"A\u00f1adiendo Memoria Persistente para Chatbots","text":"<p>En el ejemplo anterior, la cadena procesa una lista plana de mensajes sin mantener el estado entre invocaciones:</p> <pre><code>chain = (\n    RunnableLambda(trim_chat_history)  # Recorta los mensajes directamente\n    | prompt  # Formatea los mensajes recortados en el prompt\n    | llm  # Genera una respuesta usando el LLM\n)\n</code></pre> <p>Para a\u00f1adir memoria persistente, envolveremos esta cadena con <code>RunnableWithMessageHistory</code>. Al implementar <code>RunnableWithMessageHistory</code> en LCEL, necesitamos definir una pipeline de base, y esta a su vez se define a partir de una plantilla de prompt y un LLM. Comencemos definiendo un nuevo prompt template; en este caso, usaremos <code>from_message</code>, para lo cual importaremos los m\u00f3dulos:</p> <pre><code>from langchain.prompts import (\n    SystemMessagePromptTemplate, \n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    ChatPromptTemplate\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n</code></pre> <p>Dividiremos el proceso en los siguientes pasos:</p> <ol> <li> <p>Crear la pipeline base. Como lo hemos hecho antes, definimos el prompt y un LLM. Por ejemplo:</p> <pre><code># Definir el prompt del sistema al estilo de Don Quijote\nsystem_prompt = \"Eres un asistente \u00fatil que responde en una sola oraci\u00f3n en espa\u00f1ol, al estilo de Don Quijote de la Mancha.\"\n\n# Crear la plantilla de prompt para el chat\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    MessagesPlaceholder(variable_name=\"history\"),\n    HumanMessagePromptTemplate.from_template(\"{query}\"),\n])\n\n# Inicializar el LLM\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# Crear la pipeline base\npipeline = prompt_template | llm\n</code></pre> <p>Esta es nuestra pipeline base sobre la cual a\u00f1adiremos memoria usando <code>RunnableWithMessageHistory</code> y de esta forma retener el historial de conversaciones. Necesitaremos configurar un almac\u00e9n de historial de chat, puede ser un simple diccionario:</p> </li> <li> <p>Configurar la Gesti\u00f3n del Historial de Chat. Para a\u00f1adir memoria, necesitamos un mecanismo para almacenar y recuperar el historial de conversaciones para diferentes sesiones de usuario. Nuestro <code>RunnableWithMessageHistory</code> requiere que nuestra pipeline est\u00e9 envuelta en un objeto <code>RunnableWithMessageHistory</code>. Este objeto necesita algunos par\u00e1metros de entrada. Uno de ellos es <code>get_session_history</code>, que requiere una funci\u00f3n que devuelva un objeto <code>ChatMessageHistory</code> basado en un ID de sesi\u00f3n. Definimos esta funci\u00f3n nosotros mismos:</p> <pre><code># Definir el historial de chat\nchat_history = [\n    HumanMessage(content=\"Saludos, mi nombre es Juan, \u00bfc\u00f3mo te hallas?\"),\n    AIMessage(content=\"En verdad, me hallo en buen estado, y estoy presto a servirte, \u00bfcu\u00e1l es tu deseo?\"),\n    HumanMessage(content=\"\u00bfPuedes hablarme del tiempo que hoy nos acompa\u00f1a?\"),\n    AIMessage(content=\"Ciertamente, el d\u00eda se muestra soleado con un calor apacible que alcanza los 24 grados.\"),\n    HumanMessage(content=\"\u00bfCu\u00e1l es la capital de Francia?\"),\n    AIMessage(content=\"La capital de Francia es Par\u00eds, noble villa de gran renombre.\"),\n]\n\n# Configurar la gesti\u00f3n del historial de chat con un historial predefinido\nchat_dict = {}\nsession_id = \"id_123\"\nchat_dict[session_id] = InMemoryChatMessageHistory()\nchat_dict[session_id].add_messages(chat_history)\n\ndef get_chat_history(session_id: str) -&gt; InMemoryChatMessageHistory:\n    if session_id not in chat_dict:\n        chat_dict[session_id] = InMemoryChatMessageHistory()\n    return chat_dict[session_id]\n</code></pre> <p><code>chat_dict</code> es un diccionario que asigna IDs de sesi\u00f3n a objetos <code>InMemoryChatMessageHistory</code>, los cuales almacenan secuencias de mensajes (por ejemplo, mensajes del usuario y respuestas del asistente).</p> <p><code>get_chat_history</code> es una funci\u00f3n que: - Toma un <code>session_id</code> (por ejemplo, \"id_123\") como entrada. - Verifica si existe un historial de chat para ese <code>session_id</code> en <code>chat_dict</code>. - Si no, crea un nuevo <code>InMemoryChatMessageHistory</code> y lo almacena en <code>chat_dict</code>. - Luego, devuelve el objeto de historial de chat correspondiente.</p> <p>Para aprender m\u00e1s</p> <p>Esta configuraci\u00f3n asegura que cada sesi\u00f3n de usuario tenga su propio historial de chat aislado, permitiendo que m\u00faltiples usuarios interact\u00faen simult\u00e1neamente sin mezclar conversaciones. Sin embargo, para entornos de producci\u00f3n, el almacenamiento en memoria no es ideal debido a su volatilidad (los datos se pierden al reiniciar la aplicaci\u00f3n). Las alternativas incluyen: - <code>RedisChatMessageHistory</code> para almacenamiento escalable y persistente con Redis. - <code>PostgresChatMessageHistory</code> para almacenamiento respaldado por bases de datos. - <code>FileChatMessageHistory</code> para persistencia basada en archivos.</p> <p>Por ejemplo, para usar Redis, debes usar el paquete Redis (<code>%pip install --upgrade --quiet redis</code>), iniciar un servidor Redis (por ejemplo, a trav\u00e9s de Docker) y definir un <code>get_message_history</code> con <code>RedisChatMessageHistory</code>, como se muestra en la documentaci\u00f3n de LangChain sobre Integraciones de Memoria.</p> </li> <li> <p>Envolver la Pipeline Base con <code>RunnableWithMessageHistory</code>. Para a\u00f1adir memoria, envolvemos la pipeline base con <code>RunnableWithMessageHistory</code>, que gestiona el historial de chat para el <code>Runnable</code>:</p> <pre><code># Envolver la pipeline con RunnableWithMessageHistory\npipeline_with_history = RunnableWithMessageHistory(\n    pipeline,\n    get_session_history=get_chat_history,\n    input_messages_key=\"query\",\n    history_messages_key=\"history\"\n)\n</code></pre> <p>Aqu\u00ed, <code>pipeline</code> es el <code>Runnable</code> base que vamos a envolver (en este caso, <code>prompt_template | llm</code>). <code>get_session_history</code> es la funci\u00f3n (<code>get_chat_history</code>) que devuelve el historial de chat para un ID de sesi\u00f3n dado. Esta funci\u00f3n debe tomar un <code>session_id</code> y devolver una instancia de <code>BaseChatMessageHistory</code>. <code>input_messages_key</code> especifica la clave en el diccionario de entrada que contiene el mensaje actual del usuario. En el c\u00f3digo, es \"query\", lo que significa que la entrada espera algo como <code>{\"query\": \"What is my name again?\"}</code>. <code>history_messages_key</code> especifica la clave donde el historial de la conversaci\u00f3n debe ser inyectado en la entrada. En el c\u00f3digo, es \"history\", lo que significa que el historial (una lista de objetos <code>BaseMessage</code>) se a\u00f1ade bajo esta variable de entrada.</p> <p>Al invocar, <code>RunnableWithMessageHistory</code> recupera el historial de chat para el ID de sesi\u00f3n usando <code>get_chat_history</code>. Aumenta el diccionario de entrada a\u00f1adiendo el historial bajo la clave <code>history_messages_key</code> (por ejemplo, <code>{\"query\": \"What is my name again?\", \"history\": [...]}</code>). La entrada aumentada se pasa a la pipeline base, que incluye el historial en el prompt a trav\u00e9s de <code>MessagesPlaceholder</code>. Despu\u00e9s de que la pipeline genera una respuesta, el historial de chat se actualiza con el nuevo mensaje del usuario y la respuesta del asistente.</p> </li> </ol> <p>Y listo, nuestro historial de chat ahora se memorizar\u00e1 y recuperar\u00e1 cada vez que invoquemos nuestro runnable con el mismo ID de sesi\u00f3n.</p> C\u00f3digoSalida <pre><code># Invocar la pipeline para demostrar la memoria\nresult1 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfCu\u00e1l es mi nombre?\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result1.content)  # Esperado: \"Vuestro nombre, seg\u00fan me hab\u00e9is dicho, es Juan.\"\n\nresult2 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfQu\u00e9 m\u00e1s sabes de Francia?\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result2.content)\n</code></pre> <pre><code>Vuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.\nFrancia es tierra de exquisita gastronom\u00eda, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.\n</code></pre> <p>Nuestro chat ahora tiene la capacidad de recordar todas las interacciones. Vemos:</p> <pre><code>result2 = pipeline_with_history.invoke(\n    {\"query\": \"El nombre de mi madre es Maria\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result2.content) \n\nresult2 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfQui\u00e9n es Maria en mi vida?\"},\n    config={\"session_id\": \"id_123\"}\n)\n\nchat = get_chat_history('id_123')\n\nfor msg in chat.messages:\n    print(msg.content)\n</code></pre> Salida <pre><code>Saludos, mi nombre es Juan, \u00bfc\u00f3mo te hallas?\nEn verdad, me hallo en buen estado, y estoy presto a servirte, \u00bfcu\u00e1l es tu deseo?\n\u00bfPuedes hablarme del tiempo que hoy nos acompa\u00f1a?\nCiertamente, el d\u00eda se muestra soleado con un calor apacible que alcanza los 24 grados.\n\u00bfCu\u00e1l es la capital de Francia?\nLa capital de Francia es Par\u00eds, noble villa de gran renombre.\n\u00bfCu\u00e1l es mi nombre?\nVuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.\n\u00bfQu\u00e9 m\u00e1s sabes de Francia?\nFrancia es tierra de exquisita gastronom\u00eda, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.\nEl nombre de mi madre es Maria\nVuestra madre, Mar\u00eda, posee un nombre tan puro y bello como el de la Virgen Santa.\n\u00bfQui\u00e9n es Maria?\nMar\u00eda es un nombre com\u00fan entre las mujeres, pero tambi\u00e9n es el nombre de la madre de Jes\u00fas, la Virgen Mar\u00eda, figura importante en la religi\u00f3n cat\u00f3lica.\n\u00bfQui\u00e9n es Maria en mi vida?\nMar\u00eda, en vuestra vida, es la mujer que os dio la vida, os cuid\u00f3 con amor y os gui\u00f3 en vuestro camino, como una luz en la oscuridad.\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Hasta aqu\u00ed hemos reproducido el comportamiento de la clase <code>ConversationBufferMemory</code> que describimos al principio. Sin embargo, esta clase ser\u00e1 deprecada en las versiones futuras de LangChain. Para ver c\u00f3mo crear wrappers de los dem\u00e1s tipos de memoria, puedes consultar el siguiente material: Art\u00edculo: Introducci\u00f3n a los Tipos de Memoria en LangChain URL: https://www.aurelio.ai/learn/langchain-memory-types</p>"},{"location":"Unidad2/unidad2/#evidencia-de-aprendizaje","title":"Evidencia de Aprendizaje","text":"Unidad 1 Cadenas y Memoria EA1. **Generaci\u00f3n de Informes de Salud Utilizando Archivos CSV ** <p>En este proyecto practicar\u00e1s el uso de cadenas para desrrollar un sistema que reliza tareas secuenciales y ramificadas, cargar\u00e1s los resitado de estas operacones en el bufer de memoria de un chatbot.</p>"},{"location":"Unidad2/unidad2/#instrucciones","title":"Instrucciones","text":"<p>Descarga el archivo healthcare_report.csv proporcionado y, usando LECL, desarrolla un sistema capaz de: 1. Procesar un informe de salud original en espa\u00f1ol: Lee el informe desde el archivo CSV. 2. Traducir el informe al ingl\u00e9s: Usa una cadena para traducir el texto. 3. Resumir el informe traducido: Genera un resumen breve en ingl\u00e9s. 4. Extraer indicadores clave de salud del resumen: Identifica elementos clave (e.g., s\u00edntomas, duraci\u00f3n). 5. Generar un plan de tratamiento basado en los indicadores clave: Prop\u00f3n pasos de tratamiento. 6. Detectar el idioma original del informe: Determina si el informe original est\u00e1 en espa\u00f1ol. 7. Generar una recomendaci\u00f3n de seguimiento en el idioma detectado: Devuelve una recomendaci\u00f3n en espa\u00f1ol.</p> <p>Finalmete carga el infome m\u00e9dico del paciente en la memoria y crea un chat bot que est\u00e9 en capacidad de responder preguntas sobre el tratamiento indicado. Demuestra su uso con algunas llamas al chat</p> <p>Guarda los documentos con la siguiente nomenclatura:</p> <ul> <li>Apellido_Nombre del estudiante.ipynb Ejemplo: </li> <li>L\u00f3pez_Karla.ipynb</li> </ul> <p>Finalmente, haz clic en el bot\u00f3n Cargar Tarea, sube tu archivo y presiona el bot\u00f3n Enviar para remitirlo a tu profesor con el fin de que lo eval\u00fae y retroalimente. |</p> <p>\ud83d\udcd6 Nota</p> <p>Conoce los criterios de evaluaci\u00f3n de esta evidencia de aprendizaje consultando la r\u00fabrica que encontrar\u00e1s a continuaci\u00f3n.</p> Criterios Ponderaci\u00f3n Totales 70 50 5 0 Calidad de las Soluciones Las soluciones a los ejercicios son correctas, demostrando una implementaci\u00f3n adecuada de los conceptos y t\u00e9cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci\u00f3n adecuada de los conceptos y t\u00e9cnicas involucradas. Hay evidencia de esfuerzo y comprensi\u00f3n de los temas. Las soluciones presentadas son en su mayor\u00eda incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi\u00f3n de los conceptos y t\u00e9cnicas esenciales. No realiza la entrega 70 Calidad de la entrega El notebook es claro y f\u00e1cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c\u00f3digo en las celdas Markdown, lo que facilita la comprensi\u00f3n de las soluciones propuestas. El notebook no es particularmente f\u00e1cil de leer, pero a\u00fan as\u00ed incluye comentarios que explican el funcionamiento del c\u00f3digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l\u00f3gica detr\u00e1s del c\u00f3digo. El notebook carece de comentarios acerca del funcionamiento del c\u00f3digo en las celdas Markdown, lo que dificulta la comprensi\u00f3n de las soluciones implementadas. No realiza la entrega 20 Tiempo de la entrega La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci\u00f3n de la actividad. La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. La entrega se realiza con m\u00e1s de una semana de atraso, lo que indica un retraso significativo en la presentaci\u00f3n de la actividad. No realiza la entrega 10 Ponderaci\u00f3n de la actividad 100 puntos"},{"location":"Unidad2/unidad2/#referencias","title":"Referencias","text":"<p>Aurelio AI. (s.f.). LangChain Course. Recuperado el 21 de mayo de 2025, de https://www.aurelio.ai/course/langchain</p> <p>Chase, H., &amp; Ng, A. (2023). LangChain for LLM Application Development [Curso en l\u00ednea]. DeepLearning.AI. Disponible en https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</p>"},{"location":"Unidad2/unidad2/#lecturas-y-material-complementario","title":"Lecturas y material complementario","text":""},{"location":"Unidad2/unidad2/#lecturas-recomendadas","title":"\ud83d\udcda Lecturas recomendadas","text":""},{"location":"Unidad2/unidad2/#titulo-how-trellix-uses-langchain-to-enhance-cybersecurity","title":"T\u00edtulo: How Trellix Uses LangChain to Enhance Cybersecurity","text":"<p>Autor: [LangChain] Fecha de recuperaci\u00f3n: 21 de mayo de 2025 URL: How Trellix Uses LangChain to Enhance Cybersecurity</p>"},{"location":"Unidad2/unidad2/#videos-recomendados","title":"\ud83c\udfa5 Videos recomendados","text":""},{"location":"Unidad2/unidad2/#titulo-langchain-prompts-parsers-and-chaining-for-beginners","title":"T\u00edtulo: LangChain: Prompts, Parsers and Chaining | for Beginners","text":"<p>Autor: [Anub Gupta on Learn4Tarakki] URL: LangChain: Prompts, Parsers and Chaining | for Beginners Este video ofrece una introducci\u00f3n amigable para principiantes sobre c\u00f3mo crear plantillas de prompts, utilizar parsers y encadenar componentes en LangChain.</p>"},{"location":"Unidad2/unidad2/#titulo-interrupt-2025-keynote-harrison-chase-langchain","title":"T\u00edtulo: Interrupt 2025 Keynote | Harrison Chase | LangChain","text":"<p>Autor: [LangChain] URL: Interrupt 2025 Keynote | Harrison Chase | LangChain</p> <p>Este video presenta la keynote de Harrison Chase en la conferencia Interrupt 2025 de LangChain, donde se discute la evoluci\u00f3n de la ingenier\u00eda de agentes y la visi\u00f3n de la compa\u00f1\u00eda para agentes inteligentes. Incluye reflexiones sobre la trayectoria de LangChain y anuncios de nuevas herramientas de desarrollo.</p>"},{"location":"assets/admonitions/","title":"Admonitions","text":"<p>Para tener en cuenta</p> <p>Aseg\u00farate de evaluar los posibles sesgos en los datos antes de implementar un modelo de IA. Los sesgos no detectados pueden llevar a decisiones injustas, afectando la equidad y la confianza en la tecnolog\u00eda.</p> <p>-</p> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Si deseas conocer m\u00e1s sobre [tema], lee el siguiente material: Art\u00edculo de [nombre del art\u00edculo]: URL: [enlace]</p> <ul> <li> Video: Oportunidades en IA Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Sabias que Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Reto formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Recurso formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> Divisi\u00f3n en tokens de una frase utilizando el tokenizador de GPT-4. Fuente: ."}]}